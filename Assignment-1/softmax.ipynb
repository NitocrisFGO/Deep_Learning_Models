{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This mounts your Google Drive to the Colab VM.\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# # TODO: Enter the foldername in your Drive where you have saved the unzipped\n",
    "# # assignment folder, e.g. 'cs6353/assignments/assignment1/'\n",
    "# FOLDERNAME = 'assignment1'\n",
    "# assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
    "\n",
    "# # Now that we've mounted your Drive, this ensures that\n",
    "# # the Python interpreter of the Colab VM can load\n",
    "# # python files from within it.\n",
    "# import sys\n",
    "# sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n",
    "\n",
    "# # This downloads the CIFAR-10 dataset to your Drive\n",
    "# # if it doesn't already exist.\n",
    "# %cd /content/drive/My\\ Drive/$FOLDERNAME/cs6353/datasets/\n",
    "# !bash get_datasets.sh\n",
    "# %cd /content/drive/My\\ Drive/$FOLDERNAME\n",
    "\n",
    "# # Install requirements from colab_requirements.txt\n",
    "# # TODO: Please change your path below to the colab_requirements.txt file\n",
    "# ! python -m pip install -r /content/drive/My\\ Drive/$FOLDERNAME/colab_requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax exercise\n",
    "\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](https://utah.instructure.com/courses/919972/assignments/12590082) on the course website.*\n",
    "\n",
    "This exercise is analogous to the SVM exercise. You will:\n",
    "\n",
    "- implement a fully-vectorized **loss function** for the Softmax classifier\n",
    "- implement the fully-vectorized expression for its **analytic gradient**\n",
    "- **check your implementation** with numerical gradient\n",
    "- use a validation set to **tune the learning rate and regularization** strength\n",
    "- **optimize** the loss function with **SGD**\n",
    "- **visualize** the final learned weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-24T23:26:31.225559800Z",
     "start_time": "2023-09-24T23:26:30.748063700Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import random\n",
    "import numpy as np\n",
    "from cs6353.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-24T23:26:35.773382100Z",
     "start_time": "2023-09-24T23:26:33.848805900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3073)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3073)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3073)\n",
      "Test labels shape:  (1000,)\n",
      "dev data shape:  (500, 3073)\n",
      "dev labels shape:  (500,)\n"
     ]
    }
   ],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the linear classifier. These are the same steps as we used for the\n",
    "    SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'cs6353/datasets/cifar-10-batches-py'\n",
    "    \n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "    \n",
    "    # subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "    X_dev = X_train[mask]\n",
    "    y_dev = y_train[mask]\n",
    "    \n",
    "    # Preprocessing: reshape the image data into rows\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "    \n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis = 0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "    X_dev -= mean_image\n",
    "    \n",
    "    # add bias dimension and transform into columns\n",
    "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "    X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "\n",
    "# Cleaning up variables to prevent loading data multiple times (which may cause memory issue)\n",
    "try:\n",
    "   del X_train, y_train\n",
    "   del X_test, y_test\n",
    "   print('Clear previously loaded data.')\n",
    "except:\n",
    "   pass\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)\n",
    "print('dev labels shape: ', y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Classifier\n",
    "\n",
    "Your code for this section will all be written inside **cs6353/classifiers/softmax.py**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-24T23:26:37.385115200Z",
     "start_time": "2023-09-24T23:26:37.290483500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.398060\n",
      "sanity check: 2.302585\n"
     ]
    }
   ],
   "source": [
    "# First implement the naive softmax loss function with nested loops.\n",
    "# Open the file cs6353/classifiers/softmax.py and implement the\n",
    "# softmax_loss_naive function.\n",
    "\n",
    "from cs6353.classifiers.softmax import softmax_loss_naive\n",
    "import time\n",
    "\n",
    "# Generate a random softmax weight matrix and use it to compute the loss.\n",
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print('loss: %f' % loss)\n",
    "print('sanity check: %f' % (-np.log(0.1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inline Question 1:\n",
    "Why do we expect our loss to be close to -log(0.1)? Explain briefly.**\n",
    "\n",
    "**Your answer:** \n",
    "Because for the weights without gradient descent, it can be roughly estimated that the probability of selecting each category is 1/10, so the probability of selecting the correct category is 1/10. The loss function can be expressed as 1/N * sigma (-log (1 /10)), its value is -log(0.1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-24T23:36:06.261791500Z",
     "start_time": "2023-09-24T23:36:04.504617500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: -0.672178 analytic: -0.672178, relative error: 4.682864e-08\n",
      "numerical: 6.184807 analytic: 6.184807, relative error: 1.222230e-08\n",
      "numerical: 0.186668 analytic: 0.186668, relative error: 1.368333e-08\n",
      "numerical: -1.766186 analytic: -1.766186, relative error: 3.407252e-08\n",
      "numerical: -3.437839 analytic: -3.437839, relative error: 2.103722e-08\n",
      "numerical: -2.310615 analytic: -2.310615, relative error: 7.520068e-09\n",
      "numerical: -0.041020 analytic: -0.041020, relative error: 2.381192e-07\n",
      "numerical: -2.044117 analytic: -2.044117, relative error: 1.198714e-08\n",
      "numerical: -0.944077 analytic: -0.944077, relative error: 8.487483e-09\n",
      "numerical: -0.915686 analytic: -0.915686, relative error: 6.271561e-08\n",
      "numerical: 0.622275 analytic: 0.622275, relative error: 4.265229e-08\n",
      "numerical: -2.839514 analytic: -2.839514, relative error: 4.901588e-09\n",
      "numerical: 0.842350 analytic: 0.842350, relative error: 7.881849e-08\n",
      "numerical: 2.091051 analytic: 2.091051, relative error: 3.783442e-08\n",
      "numerical: 1.650637 analytic: 1.650637, relative error: 2.544628e-09\n",
      "numerical: 0.583131 analytic: 0.583131, relative error: 1.168748e-08\n",
      "numerical: 0.776396 analytic: 0.776396, relative error: 5.608973e-08\n",
      "numerical: 1.974616 analytic: 1.974616, relative error: 5.357516e-09\n",
      "numerical: 2.304291 analytic: 2.304291, relative error: 2.288593e-08\n",
      "numerical: 0.606504 analytic: 0.606504, relative error: 9.107837e-08\n"
     ]
    }
   ],
   "source": [
    "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
    "# version of the gradient that uses nested loops.\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As we did for the SVM, use numeric gradient checking as a debugging tool.\n",
    "# The numeric gradient should be close to the analytic gradient.\n",
    "from cs6353.gradient_check import grad_check_sparse\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
    "\n",
    "# similar to SVM case, do another gradient check with regularization\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 5e1)\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 5e1)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T00:20:09.759914300Z",
     "start_time": "2023-09-25T00:20:09.640726600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive loss: 2.398060e+00 computed in 0.049833s\n",
      "vectorized loss: 2.398060e+00 computed in 0.001995s\n",
      "Loss difference: 0.000000\n",
      "Gradient difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now that we have a naive implementation of the softmax loss function and its gradient,\n",
    "# implement a vectorized version in softmax_loss_vectorized.\n",
    "# The two versions should compute the same results, but the vectorized version should be\n",
    "# much faster.\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n",
    "\n",
    "from cs6353.classifiers.softmax import softmax_loss_vectorized\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n",
    "\n",
    "# As we did for the SVM, we use the Frobenius norm to compare the two versions\n",
    "# of the gradient.\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print('Loss difference: %f' % np.abs(loss_naive - loss_vectorized))\n",
    "print('Gradient difference: %f' % grad_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T00:31:45.828242800Z",
     "start_time": "2023-09-25T00:27:05.672491200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 1500: loss 464.768182\n",
      "iteration 100 / 1500: loss 286.698064\n",
      "iteration 200 / 1500: loss 177.403418\n",
      "iteration 300 / 1500: loss 110.294864\n",
      "iteration 400 / 1500: loss 69.049094\n",
      "iteration 500 / 1500: loss 43.236798\n",
      "iteration 600 / 1500: loss 27.565002\n",
      "iteration 700 / 1500: loss 17.752926\n",
      "iteration 800 / 1500: loss 11.722581\n",
      "iteration 900 / 1500: loss 8.017363\n",
      "iteration 1000 / 1500: loss 5.702425\n",
      "iteration 1100 / 1500: loss 4.319076\n",
      "iteration 1200 / 1500: loss 3.529038\n",
      "iteration 1300 / 1500: loss 2.914610\n",
      "iteration 1400 / 1500: loss 2.609000\n",
      "iteration 0 / 1500: loss 554.824261\n",
      "iteration 100 / 1500: loss 313.484443\n",
      "iteration 200 / 1500: loss 177.729218\n",
      "iteration 300 / 1500: loss 101.170909\n",
      "iteration 400 / 1500: loss 58.176490\n",
      "iteration 500 / 1500: loss 33.683133\n",
      "iteration 600 / 1500: loss 19.899195\n",
      "iteration 700 / 1500: loss 12.156976\n",
      "iteration 800 / 1500: loss 7.766400\n",
      "iteration 900 / 1500: loss 5.292095\n",
      "iteration 1000 / 1500: loss 3.828682\n",
      "iteration 1100 / 1500: loss 3.187020\n",
      "iteration 1200 / 1500: loss 2.577394\n",
      "iteration 1300 / 1500: loss 2.375488\n",
      "iteration 1400 / 1500: loss 2.253092\n",
      "iteration 0 / 1500: loss 633.846836\n",
      "iteration 100 / 1500: loss 327.290989\n",
      "iteration 200 / 1500: loss 170.149965\n",
      "iteration 300 / 1500: loss 88.752268\n",
      "iteration 400 / 1500: loss 46.773606\n",
      "iteration 500 / 1500: loss 25.226816\n",
      "iteration 600 / 1500: loss 14.025741\n",
      "iteration 700 / 1500: loss 8.287788\n",
      "iteration 800 / 1500: loss 5.290053\n",
      "iteration 900 / 1500: loss 3.701510\n",
      "iteration 1000 / 1500: loss 2.959902\n",
      "iteration 1100 / 1500: loss 2.518391\n",
      "iteration 1200 / 1500: loss 2.309659\n",
      "iteration 1300 / 1500: loss 2.171430\n",
      "iteration 1400 / 1500: loss 2.158646\n",
      "iteration 0 / 1500: loss 716.833864\n",
      "iteration 100 / 1500: loss 339.064600\n",
      "iteration 200 / 1500: loss 161.051337\n",
      "iteration 300 / 1500: loss 77.205590\n",
      "iteration 400 / 1500: loss 37.512041\n",
      "iteration 500 / 1500: loss 18.892142\n",
      "iteration 600 / 1500: loss 9.997532\n",
      "iteration 700 / 1500: loss 5.812813\n",
      "iteration 800 / 1500: loss 3.873121\n",
      "iteration 900 / 1500: loss 2.935442\n",
      "iteration 1000 / 1500: loss 2.473535\n",
      "iteration 1100 / 1500: loss 2.248061\n",
      "iteration 1200 / 1500: loss 2.143920\n",
      "iteration 1300 / 1500: loss 2.150002\n",
      "iteration 1400 / 1500: loss 2.048683\n",
      "iteration 0 / 1500: loss 812.891761\n",
      "iteration 100 / 1500: loss 351.896349\n",
      "iteration 200 / 1500: loss 152.982942\n",
      "iteration 300 / 1500: loss 67.338754\n",
      "iteration 400 / 1500: loss 30.262149\n",
      "iteration 500 / 1500: loss 14.251660\n",
      "iteration 600 / 1500: loss 7.344385\n",
      "iteration 700 / 1500: loss 4.364301\n",
      "iteration 800 / 1500: loss 3.039987\n",
      "iteration 900 / 1500: loss 2.551947\n",
      "iteration 1000 / 1500: loss 2.238912\n",
      "iteration 1100 / 1500: loss 2.088885\n",
      "iteration 1200 / 1500: loss 2.081459\n",
      "iteration 1300 / 1500: loss 2.117538\n",
      "iteration 1400 / 1500: loss 2.084181\n",
      "iteration 0 / 1500: loss 887.517976\n",
      "iteration 100 / 1500: loss 351.721960\n",
      "iteration 200 / 1500: loss 140.162291\n",
      "iteration 300 / 1500: loss 56.653437\n",
      "iteration 400 / 1500: loss 23.624119\n",
      "iteration 500 / 1500: loss 10.558423\n",
      "iteration 600 / 1500: loss 5.449196\n",
      "iteration 700 / 1500: loss 3.435088\n",
      "iteration 800 / 1500: loss 2.549853\n",
      "iteration 900 / 1500: loss 2.299047\n",
      "iteration 1000 / 1500: loss 2.173079\n",
      "iteration 1100 / 1500: loss 2.174168\n",
      "iteration 1200 / 1500: loss 2.119393\n",
      "iteration 1300 / 1500: loss 2.139666\n",
      "iteration 1400 / 1500: loss 2.098097\n",
      "iteration 0 / 1500: loss 982.139109\n",
      "iteration 100 / 1500: loss 355.772479\n",
      "iteration 200 / 1500: loss 129.818740\n",
      "iteration 300 / 1500: loss 48.225440\n",
      "iteration 400 / 1500: loss 18.811415\n",
      "iteration 500 / 1500: loss 8.113041\n",
      "iteration 600 / 1500: loss 4.273060\n",
      "iteration 700 / 1500: loss 2.842279\n",
      "iteration 800 / 1500: loss 2.437034\n",
      "iteration 900 / 1500: loss 2.225387\n",
      "iteration 1000 / 1500: loss 2.114433\n",
      "iteration 1100 / 1500: loss 2.107111\n",
      "iteration 1200 / 1500: loss 2.114422\n",
      "iteration 1300 / 1500: loss 2.112970\n",
      "iteration 1400 / 1500: loss 2.106965\n",
      "iteration 0 / 1500: loss 1060.172995\n",
      "iteration 100 / 1500: loss 350.991114\n",
      "iteration 200 / 1500: loss 117.388988\n",
      "iteration 300 / 1500: loss 40.212523\n",
      "iteration 400 / 1500: loss 14.702280\n",
      "iteration 500 / 1500: loss 6.311611\n",
      "iteration 600 / 1500: loss 3.532658\n",
      "iteration 700 / 1500: loss 2.574163\n",
      "iteration 800 / 1500: loss 2.231367\n",
      "iteration 900 / 1500: loss 2.194532\n",
      "iteration 1000 / 1500: loss 2.160525\n",
      "iteration 1100 / 1500: loss 2.096662\n",
      "iteration 1200 / 1500: loss 2.132063\n",
      "iteration 1300 / 1500: loss 2.138003\n",
      "iteration 1400 / 1500: loss 2.133870\n",
      "iteration 0 / 1500: loss 1146.002206\n",
      "iteration 100 / 1500: loss 347.233953\n",
      "iteration 200 / 1500: loss 106.351839\n",
      "iteration 300 / 1500: loss 33.631259\n",
      "iteration 400 / 1500: loss 11.614714\n",
      "iteration 500 / 1500: loss 4.944960\n",
      "iteration 600 / 1500: loss 2.942938\n",
      "iteration 700 / 1500: loss 2.394537\n",
      "iteration 800 / 1500: loss 2.184364\n",
      "iteration 900 / 1500: loss 2.112868\n",
      "iteration 1000 / 1500: loss 2.095780\n",
      "iteration 1100 / 1500: loss 2.143291\n",
      "iteration 1200 / 1500: loss 2.119253\n",
      "iteration 1300 / 1500: loss 2.077600\n",
      "iteration 1400 / 1500: loss 2.147264\n",
      "iteration 0 / 1500: loss 1243.643808\n",
      "iteration 100 / 1500: loss 344.732867\n",
      "iteration 200 / 1500: loss 96.699653\n",
      "iteration 300 / 1500: loss 28.234977\n",
      "iteration 400 / 1500: loss 9.293539\n",
      "iteration 500 / 1500: loss 4.094677\n",
      "iteration 600 / 1500: loss 2.674493\n",
      "iteration 700 / 1500: loss 2.273210\n",
      "iteration 800 / 1500: loss 2.190165\n",
      "iteration 900 / 1500: loss 2.139674\n",
      "iteration 1000 / 1500: loss 2.172040\n",
      "iteration 1100 / 1500: loss 2.141595\n",
      "iteration 1200 / 1500: loss 2.141884\n",
      "iteration 1300 / 1500: loss 2.151340\n",
      "iteration 1400 / 1500: loss 2.137915\n",
      "iteration 0 / 1500: loss 468.695781\n",
      "iteration 100 / 1500: loss 157.073942\n",
      "iteration 200 / 1500: loss 53.651679\n",
      "iteration 300 / 1500: loss 19.173686\n",
      "iteration 400 / 1500: loss 7.700028\n",
      "iteration 500 / 1500: loss 3.912463\n",
      "iteration 600 / 1500: loss 2.569832\n",
      "iteration 700 / 1500: loss 2.249175\n",
      "iteration 800 / 1500: loss 2.075259\n",
      "iteration 900 / 1500: loss 2.044183\n",
      "iteration 1000 / 1500: loss 2.153326\n",
      "iteration 1100 / 1500: loss 1.986222\n",
      "iteration 1200 / 1500: loss 2.050826\n",
      "iteration 1300 / 1500: loss 2.017789\n",
      "iteration 1400 / 1500: loss 2.050729\n",
      "iteration 0 / 1500: loss 549.270756\n",
      "iteration 100 / 1500: loss 150.178634\n",
      "iteration 200 / 1500: loss 42.174028\n",
      "iteration 300 / 1500: loss 12.943574\n",
      "iteration 400 / 1500: loss 4.977956\n",
      "iteration 500 / 1500: loss 2.831300\n",
      "iteration 600 / 1500: loss 2.381308\n",
      "iteration 700 / 1500: loss 2.160887\n",
      "iteration 800 / 1500: loss 2.054446\n",
      "iteration 900 / 1500: loss 2.032254\n",
      "iteration 1000 / 1500: loss 1.999281\n",
      "iteration 1100 / 1500: loss 2.010083\n",
      "iteration 1200 / 1500: loss 2.054578\n",
      "iteration 1300 / 1500: loss 2.023151\n",
      "iteration 1400 / 1500: loss 2.120635\n",
      "iteration 0 / 1500: loss 632.464719\n",
      "iteration 100 / 1500: loss 141.020580\n",
      "iteration 200 / 1500: loss 32.779895\n",
      "iteration 300 / 1500: loss 8.870639\n",
      "iteration 400 / 1500: loss 3.658188\n",
      "iteration 500 / 1500: loss 2.402646\n",
      "iteration 600 / 1500: loss 2.156470\n",
      "iteration 700 / 1500: loss 2.038050\n",
      "iteration 800 / 1500: loss 2.045151\n",
      "iteration 900 / 1500: loss 2.101272\n",
      "iteration 1000 / 1500: loss 2.077290\n",
      "iteration 1100 / 1500: loss 2.157673\n",
      "iteration 1200 / 1500: loss 2.035729\n",
      "iteration 1300 / 1500: loss 2.079979\n",
      "iteration 1400 / 1500: loss 2.110386\n",
      "iteration 0 / 1500: loss 725.939278\n",
      "iteration 100 / 1500: loss 132.486968\n",
      "iteration 200 / 1500: loss 25.642880\n",
      "iteration 300 / 1500: loss 6.326133\n",
      "iteration 400 / 1500: loss 2.890985\n",
      "iteration 500 / 1500: loss 2.219470\n",
      "iteration 600 / 1500: loss 2.039387\n",
      "iteration 700 / 1500: loss 2.051384\n",
      "iteration 800 / 1500: loss 2.029789\n",
      "iteration 900 / 1500: loss 2.150457\n",
      "iteration 1000 / 1500: loss 2.119879\n",
      "iteration 1100 / 1500: loss 2.103512\n",
      "iteration 1200 / 1500: loss 2.102287\n",
      "iteration 1300 / 1500: loss 2.095328\n",
      "iteration 1400 / 1500: loss 2.050472\n",
      "iteration 0 / 1500: loss 804.980970\n",
      "iteration 100 / 1500: loss 119.870146\n",
      "iteration 200 / 1500: loss 19.338106\n",
      "iteration 300 / 1500: loss 4.641577\n",
      "iteration 400 / 1500: loss 2.451336\n",
      "iteration 500 / 1500: loss 2.194118\n",
      "iteration 600 / 1500: loss 2.102567\n",
      "iteration 700 / 1500: loss 2.093923\n",
      "iteration 800 / 1500: loss 2.093202\n",
      "iteration 900 / 1500: loss 2.107619\n",
      "iteration 1000 / 1500: loss 2.048011\n",
      "iteration 1100 / 1500: loss 2.115489\n",
      "iteration 1200 / 1500: loss 2.090407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1300 / 1500: loss 2.121482\n",
      "iteration 1400 / 1500: loss 2.043491\n",
      "iteration 0 / 1500: loss 890.732222\n",
      "iteration 100 / 1500: loss 108.291266\n",
      "iteration 200 / 1500: loss 14.814257\n",
      "iteration 300 / 1500: loss 3.576812\n",
      "iteration 400 / 1500: loss 2.296516\n",
      "iteration 500 / 1500: loss 2.096252\n",
      "iteration 600 / 1500: loss 2.106150\n",
      "iteration 700 / 1500: loss 2.117343\n",
      "iteration 800 / 1500: loss 2.092747\n",
      "iteration 900 / 1500: loss 2.067513\n",
      "iteration 1000 / 1500: loss 2.100020\n",
      "iteration 1100 / 1500: loss 2.078335\n",
      "iteration 1200 / 1500: loss 2.062025\n",
      "iteration 1300 / 1500: loss 2.084377\n",
      "iteration 1400 / 1500: loss 2.131117\n",
      "iteration 0 / 1500: loss 986.392180\n",
      "iteration 100 / 1500: loss 98.027497\n",
      "iteration 200 / 1500: loss 11.487112\n",
      "iteration 300 / 1500: loss 3.033591\n",
      "iteration 400 / 1500: loss 2.184153\n",
      "iteration 500 / 1500: loss 2.070350\n",
      "iteration 600 / 1500: loss 2.139172\n",
      "iteration 700 / 1500: loss 2.062363\n",
      "iteration 800 / 1500: loss 2.071719\n",
      "iteration 900 / 1500: loss 2.189918\n",
      "iteration 1000 / 1500: loss 2.130251\n",
      "iteration 1100 / 1500: loss 2.136630\n",
      "iteration 1200 / 1500: loss 2.054111\n",
      "iteration 1300 / 1500: loss 2.116912\n",
      "iteration 1400 / 1500: loss 2.084189\n",
      "iteration 0 / 1500: loss 1072.229887\n",
      "iteration 100 / 1500: loss 86.986798\n",
      "iteration 200 / 1500: loss 8.913776\n",
      "iteration 300 / 1500: loss 2.652950\n",
      "iteration 400 / 1500: loss 2.116437\n",
      "iteration 500 / 1500: loss 2.084276\n",
      "iteration 600 / 1500: loss 2.130919\n",
      "iteration 700 / 1500: loss 2.113057\n",
      "iteration 800 / 1500: loss 2.113139\n",
      "iteration 900 / 1500: loss 2.120281\n",
      "iteration 1000 / 1500: loss 2.167142\n",
      "iteration 1100 / 1500: loss 2.085689\n",
      "iteration 1200 / 1500: loss 2.143927\n",
      "iteration 1300 / 1500: loss 2.101340\n",
      "iteration 1400 / 1500: loss 2.087509\n",
      "iteration 0 / 1500: loss 1134.448560\n",
      "iteration 100 / 1500: loss 75.337460\n",
      "iteration 200 / 1500: loss 6.922764\n",
      "iteration 300 / 1500: loss 2.425643\n",
      "iteration 400 / 1500: loss 2.174435\n",
      "iteration 500 / 1500: loss 2.095884\n",
      "iteration 600 / 1500: loss 2.119742\n",
      "iteration 700 / 1500: loss 2.088575\n",
      "iteration 800 / 1500: loss 2.106962\n",
      "iteration 900 / 1500: loss 2.158853\n",
      "iteration 1000 / 1500: loss 2.121461\n",
      "iteration 1100 / 1500: loss 2.124882\n",
      "iteration 1200 / 1500: loss 2.117246\n",
      "iteration 1300 / 1500: loss 2.110526\n",
      "iteration 1400 / 1500: loss 2.109461\n",
      "iteration 0 / 1500: loss 1239.435096\n",
      "iteration 100 / 1500: loss 67.281834\n",
      "iteration 200 / 1500: loss 5.538626\n",
      "iteration 300 / 1500: loss 2.307317\n",
      "iteration 400 / 1500: loss 2.135517\n",
      "iteration 500 / 1500: loss 2.136133\n",
      "iteration 600 / 1500: loss 2.110058\n",
      "iteration 700 / 1500: loss 2.125631\n",
      "iteration 800 / 1500: loss 2.129607\n",
      "iteration 900 / 1500: loss 2.081190\n",
      "iteration 1000 / 1500: loss 2.146303\n",
      "iteration 1100 / 1500: loss 2.117414\n",
      "iteration 1200 / 1500: loss 2.097330\n",
      "iteration 1300 / 1500: loss 2.141065\n",
      "iteration 1400 / 1500: loss 2.142952\n",
      "iteration 0 / 1500: loss 466.605378\n",
      "iteration 100 / 1500: loss 84.824101\n",
      "iteration 200 / 1500: loss 16.899667\n",
      "iteration 300 / 1500: loss 4.711321\n",
      "iteration 400 / 1500: loss 2.552826\n",
      "iteration 500 / 1500: loss 2.180751\n",
      "iteration 600 / 1500: loss 2.083772\n",
      "iteration 700 / 1500: loss 2.034823\n",
      "iteration 800 / 1500: loss 2.013730\n",
      "iteration 900 / 1500: loss 2.036434\n",
      "iteration 1000 / 1500: loss 2.070264\n",
      "iteration 1100 / 1500: loss 2.023441\n",
      "iteration 1200 / 1500: loss 2.073898\n",
      "iteration 1300 / 1500: loss 2.057072\n",
      "iteration 1400 / 1500: loss 2.022451\n",
      "iteration 0 / 1500: loss 551.852837\n",
      "iteration 100 / 1500: loss 73.237892\n",
      "iteration 200 / 1500: loss 11.328216\n",
      "iteration 300 / 1500: loss 3.272236\n",
      "iteration 400 / 1500: loss 2.241274\n",
      "iteration 500 / 1500: loss 2.088314\n",
      "iteration 600 / 1500: loss 2.032625\n",
      "iteration 700 / 1500: loss 2.003453\n",
      "iteration 800 / 1500: loss 2.024816\n",
      "iteration 900 / 1500: loss 2.030365\n",
      "iteration 1000 / 1500: loss 2.155010\n",
      "iteration 1100 / 1500: loss 2.074692\n",
      "iteration 1200 / 1500: loss 2.070903\n",
      "iteration 1300 / 1500: loss 2.092940\n",
      "iteration 1400 / 1500: loss 2.062465\n",
      "iteration 0 / 1500: loss 645.739858\n",
      "iteration 100 / 1500: loss 62.671966\n",
      "iteration 200 / 1500: loss 7.808051\n",
      "iteration 300 / 1500: loss 2.585845\n",
      "iteration 400 / 1500: loss 2.123233\n",
      "iteration 500 / 1500: loss 2.071627\n",
      "iteration 600 / 1500: loss 2.065274\n",
      "iteration 700 / 1500: loss 2.075713\n",
      "iteration 800 / 1500: loss 1.977609\n",
      "iteration 900 / 1500: loss 2.048770\n",
      "iteration 1000 / 1500: loss 2.079371\n",
      "iteration 1100 / 1500: loss 2.074524\n",
      "iteration 1200 / 1500: loss 2.125186\n",
      "iteration 1300 / 1500: loss 2.066625\n",
      "iteration 1400 / 1500: loss 2.062682\n",
      "iteration 0 / 1500: loss 723.881707\n",
      "iteration 100 / 1500: loss 51.476759\n",
      "iteration 200 / 1500: loss 5.493343\n",
      "iteration 300 / 1500: loss 2.327577\n",
      "iteration 400 / 1500: loss 2.097159\n",
      "iteration 500 / 1500: loss 2.069976\n",
      "iteration 600 / 1500: loss 2.145041\n",
      "iteration 700 / 1500: loss 2.116711\n",
      "iteration 800 / 1500: loss 2.065275\n",
      "iteration 900 / 1500: loss 2.087515\n",
      "iteration 1000 / 1500: loss 2.159860\n",
      "iteration 1100 / 1500: loss 2.041108\n",
      "iteration 1200 / 1500: loss 2.117341\n",
      "iteration 1300 / 1500: loss 2.072963\n",
      "iteration 1400 / 1500: loss 2.088067\n",
      "iteration 0 / 1500: loss 808.678891\n",
      "iteration 100 / 1500: loss 42.181050\n",
      "iteration 200 / 1500: loss 4.058780\n",
      "iteration 300 / 1500: loss 2.226833\n",
      "iteration 400 / 1500: loss 2.076042\n",
      "iteration 500 / 1500: loss 2.046919\n",
      "iteration 600 / 1500: loss 2.114088\n",
      "iteration 700 / 1500: loss 2.133344\n",
      "iteration 800 / 1500: loss 2.056423\n",
      "iteration 900 / 1500: loss 2.080433\n",
      "iteration 1000 / 1500: loss 2.111502\n",
      "iteration 1100 / 1500: loss 2.124697\n",
      "iteration 1200 / 1500: loss 2.066333\n",
      "iteration 1300 / 1500: loss 2.039612\n",
      "iteration 1400 / 1500: loss 2.162659\n",
      "iteration 0 / 1500: loss 901.355422\n",
      "iteration 100 / 1500: loss 34.472150\n",
      "iteration 200 / 1500: loss 3.223862\n",
      "iteration 300 / 1500: loss 2.187655\n",
      "iteration 400 / 1500: loss 2.137672\n",
      "iteration 500 / 1500: loss 2.110678\n",
      "iteration 600 / 1500: loss 2.040873\n",
      "iteration 700 / 1500: loss 2.100038\n",
      "iteration 800 / 1500: loss 2.119612\n",
      "iteration 900 / 1500: loss 2.109943\n",
      "iteration 1000 / 1500: loss 2.134790\n",
      "iteration 1100 / 1500: loss 2.086122\n",
      "iteration 1200 / 1500: loss 2.150172\n",
      "iteration 1300 / 1500: loss 2.085077\n",
      "iteration 1400 / 1500: loss 2.158026\n",
      "iteration 0 / 1500: loss 973.929409\n",
      "iteration 100 / 1500: loss 27.434714\n",
      "iteration 200 / 1500: loss 2.784409\n",
      "iteration 300 / 1500: loss 2.153314\n",
      "iteration 400 / 1500: loss 2.160903\n",
      "iteration 500 / 1500: loss 2.110169\n",
      "iteration 600 / 1500: loss 2.140008\n",
      "iteration 700 / 1500: loss 2.097607\n",
      "iteration 800 / 1500: loss 2.089068\n",
      "iteration 900 / 1500: loss 2.084038\n",
      "iteration 1000 / 1500: loss 2.113159\n",
      "iteration 1100 / 1500: loss 2.128630\n",
      "iteration 1200 / 1500: loss 2.111403\n",
      "iteration 1300 / 1500: loss 2.107323\n",
      "iteration 1400 / 1500: loss 2.082243\n",
      "iteration 0 / 1500: loss 1063.725929\n",
      "iteration 100 / 1500: loss 22.211027\n",
      "iteration 200 / 1500: loss 2.502297\n",
      "iteration 300 / 1500: loss 2.074317\n",
      "iteration 400 / 1500: loss 2.105855\n",
      "iteration 500 / 1500: loss 2.130633\n",
      "iteration 600 / 1500: loss 2.128585\n",
      "iteration 700 / 1500: loss 2.068602\n",
      "iteration 800 / 1500: loss 2.159403\n",
      "iteration 900 / 1500: loss 2.116027\n",
      "iteration 1000 / 1500: loss 2.086126\n",
      "iteration 1100 / 1500: loss 2.119299\n",
      "iteration 1200 / 1500: loss 2.125860\n",
      "iteration 1300 / 1500: loss 2.101325\n",
      "iteration 1400 / 1500: loss 2.174343\n",
      "iteration 0 / 1500: loss 1151.444224\n",
      "iteration 100 / 1500: loss 17.931671\n",
      "iteration 200 / 1500: loss 2.327824\n",
      "iteration 300 / 1500: loss 2.095557\n",
      "iteration 400 / 1500: loss 2.114769\n",
      "iteration 500 / 1500: loss 2.166676\n",
      "iteration 600 / 1500: loss 2.068356\n",
      "iteration 700 / 1500: loss 2.139924\n",
      "iteration 800 / 1500: loss 2.142332\n",
      "iteration 900 / 1500: loss 2.150595\n",
      "iteration 1000 / 1500: loss 2.130873\n",
      "iteration 1100 / 1500: loss 2.049766\n",
      "iteration 1200 / 1500: loss 2.131485\n",
      "iteration 1300 / 1500: loss 2.117544\n",
      "iteration 1400 / 1500: loss 2.097603\n",
      "iteration 0 / 1500: loss 1230.929798\n",
      "iteration 100 / 1500: loss 14.278493\n",
      "iteration 200 / 1500: loss 2.257809\n",
      "iteration 300 / 1500: loss 2.134355\n",
      "iteration 400 / 1500: loss 2.174750\n",
      "iteration 500 / 1500: loss 2.199089\n",
      "iteration 600 / 1500: loss 2.128814\n",
      "iteration 700 / 1500: loss 2.121965\n",
      "iteration 800 / 1500: loss 2.103733\n",
      "iteration 900 / 1500: loss 2.138032\n",
      "iteration 1000 / 1500: loss 2.095716\n",
      "iteration 1100 / 1500: loss 2.138523\n",
      "iteration 1200 / 1500: loss 2.102196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1300 / 1500: loss 2.168366\n",
      "iteration 1400 / 1500: loss 2.049158\n",
      "iteration 0 / 1500: loss 470.937590\n",
      "iteration 100 / 1500: loss 46.928237\n",
      "iteration 200 / 1500: loss 6.396279\n",
      "iteration 300 / 1500: loss 2.455553\n",
      "iteration 400 / 1500: loss 2.119907\n",
      "iteration 500 / 1500: loss 1.952391\n",
      "iteration 600 / 1500: loss 2.022013\n",
      "iteration 700 / 1500: loss 2.010234\n",
      "iteration 800 / 1500: loss 2.094656\n",
      "iteration 900 / 1500: loss 2.035679\n",
      "iteration 1000 / 1500: loss 2.021630\n",
      "iteration 1100 / 1500: loss 2.071714\n",
      "iteration 1200 / 1500: loss 2.024700\n",
      "iteration 1300 / 1500: loss 2.136598\n",
      "iteration 1400 / 1500: loss 2.007162\n",
      "iteration 0 / 1500: loss 549.876047\n",
      "iteration 100 / 1500: loss 36.002793\n",
      "iteration 200 / 1500: loss 4.203977\n",
      "iteration 300 / 1500: loss 2.190259\n",
      "iteration 400 / 1500: loss 2.026568\n",
      "iteration 500 / 1500: loss 2.038096\n",
      "iteration 600 / 1500: loss 2.055346\n",
      "iteration 700 / 1500: loss 2.023571\n",
      "iteration 800 / 1500: loss 2.097440\n",
      "iteration 900 / 1500: loss 2.101311\n",
      "iteration 1000 / 1500: loss 2.095096\n",
      "iteration 1100 / 1500: loss 2.068668\n",
      "iteration 1200 / 1500: loss 2.065320\n",
      "iteration 1300 / 1500: loss 2.025109\n",
      "iteration 1400 / 1500: loss 2.029344\n",
      "iteration 0 / 1500: loss 633.702398\n",
      "iteration 100 / 1500: loss 27.413328\n",
      "iteration 200 / 1500: loss 3.122096\n",
      "iteration 300 / 1500: loss 2.117413\n",
      "iteration 400 / 1500: loss 2.061650\n",
      "iteration 500 / 1500: loss 2.070752\n",
      "iteration 600 / 1500: loss 2.066296\n",
      "iteration 700 / 1500: loss 2.050559\n",
      "iteration 800 / 1500: loss 2.088252\n",
      "iteration 900 / 1500: loss 2.093144\n",
      "iteration 1000 / 1500: loss 2.070022\n",
      "iteration 1100 / 1500: loss 2.006099\n",
      "iteration 1200 / 1500: loss 2.090541\n",
      "iteration 1300 / 1500: loss 2.111553\n",
      "iteration 1400 / 1500: loss 2.095188\n",
      "iteration 0 / 1500: loss 723.203197\n",
      "iteration 100 / 1500: loss 20.760593\n",
      "iteration 200 / 1500: loss 2.551764\n",
      "iteration 300 / 1500: loss 2.128096\n",
      "iteration 400 / 1500: loss 2.105540\n",
      "iteration 500 / 1500: loss 2.054644\n",
      "iteration 600 / 1500: loss 2.069411\n",
      "iteration 700 / 1500: loss 2.082874\n",
      "iteration 800 / 1500: loss 2.090700\n",
      "iteration 900 / 1500: loss 2.169140\n",
      "iteration 1000 / 1500: loss 2.106842\n",
      "iteration 1100 / 1500: loss 2.076365\n",
      "iteration 1200 / 1500: loss 2.085967\n",
      "iteration 1300 / 1500: loss 2.099697\n",
      "iteration 1400 / 1500: loss 2.031922\n",
      "iteration 0 / 1500: loss 801.567468\n",
      "iteration 100 / 1500: loss 15.475888\n",
      "iteration 200 / 1500: loss 2.268293\n",
      "iteration 300 / 1500: loss 2.052393\n",
      "iteration 400 / 1500: loss 2.054843\n",
      "iteration 500 / 1500: loss 2.064917\n",
      "iteration 600 / 1500: loss 2.047728\n",
      "iteration 700 / 1500: loss 2.127098\n",
      "iteration 800 / 1500: loss 2.094568\n",
      "iteration 900 / 1500: loss 2.106266\n",
      "iteration 1000 / 1500: loss 2.134668\n",
      "iteration 1100 / 1500: loss 2.119554\n",
      "iteration 1200 / 1500: loss 2.073591\n",
      "iteration 1300 / 1500: loss 2.102609\n",
      "iteration 1400 / 1500: loss 2.168354\n",
      "iteration 0 / 1500: loss 889.786837\n",
      "iteration 100 / 1500: loss 11.582050\n",
      "iteration 200 / 1500: loss 2.262382\n",
      "iteration 300 / 1500: loss 2.116021\n",
      "iteration 400 / 1500: loss 2.107098\n",
      "iteration 500 / 1500: loss 2.106341\n",
      "iteration 600 / 1500: loss 2.019512\n",
      "iteration 700 / 1500: loss 2.082053\n",
      "iteration 800 / 1500: loss 2.095711\n",
      "iteration 900 / 1500: loss 2.043065\n",
      "iteration 1000 / 1500: loss 2.124887\n",
      "iteration 1100 / 1500: loss 2.156159\n",
      "iteration 1200 / 1500: loss 2.078683\n",
      "iteration 1300 / 1500: loss 2.103950\n",
      "iteration 1400 / 1500: loss 2.086201\n",
      "iteration 0 / 1500: loss 979.405938\n",
      "iteration 100 / 1500: loss 8.852340\n",
      "iteration 200 / 1500: loss 2.222464\n",
      "iteration 300 / 1500: loss 2.153625\n",
      "iteration 400 / 1500: loss 2.138063\n",
      "iteration 500 / 1500: loss 2.099598\n",
      "iteration 600 / 1500: loss 2.073593\n",
      "iteration 700 / 1500: loss 2.102728\n",
      "iteration 800 / 1500: loss 2.130253\n",
      "iteration 900 / 1500: loss 2.120526\n",
      "iteration 1000 / 1500: loss 2.087761\n",
      "iteration 1100 / 1500: loss 2.123362\n",
      "iteration 1200 / 1500: loss 2.131821\n",
      "iteration 1300 / 1500: loss 2.115773\n",
      "iteration 1400 / 1500: loss 2.136262\n",
      "iteration 0 / 1500: loss 1067.966531\n",
      "iteration 100 / 1500: loss 6.822230\n",
      "iteration 200 / 1500: loss 2.164982\n",
      "iteration 300 / 1500: loss 2.179167\n",
      "iteration 400 / 1500: loss 2.155326\n",
      "iteration 500 / 1500: loss 2.124321\n",
      "iteration 600 / 1500: loss 2.152417\n",
      "iteration 700 / 1500: loss 2.117997\n",
      "iteration 800 / 1500: loss 2.098044\n",
      "iteration 900 / 1500: loss 2.090146\n",
      "iteration 1000 / 1500: loss 2.058542\n",
      "iteration 1100 / 1500: loss 2.164527\n",
      "iteration 1200 / 1500: loss 2.138257\n",
      "iteration 1300 / 1500: loss 2.139359\n",
      "iteration 1400 / 1500: loss 2.145050\n",
      "iteration 0 / 1500: loss 1147.214180\n",
      "iteration 100 / 1500: loss 5.402835\n",
      "iteration 200 / 1500: loss 2.131240\n",
      "iteration 300 / 1500: loss 2.138318\n",
      "iteration 400 / 1500: loss 2.103684\n",
      "iteration 500 / 1500: loss 2.137866\n",
      "iteration 600 / 1500: loss 2.139478\n",
      "iteration 700 / 1500: loss 2.077535\n",
      "iteration 800 / 1500: loss 2.113454\n",
      "iteration 900 / 1500: loss 2.138580\n",
      "iteration 1000 / 1500: loss 2.142717\n",
      "iteration 1100 / 1500: loss 2.101264\n",
      "iteration 1200 / 1500: loss 2.204633\n",
      "iteration 1300 / 1500: loss 2.157272\n",
      "iteration 1400 / 1500: loss 2.143970\n",
      "iteration 0 / 1500: loss 1234.120468\n",
      "iteration 100 / 1500: loss 4.386650\n",
      "iteration 200 / 1500: loss 2.125198\n",
      "iteration 300 / 1500: loss 2.133302\n",
      "iteration 400 / 1500: loss 2.134907\n",
      "iteration 500 / 1500: loss 2.121614\n",
      "iteration 600 / 1500: loss 2.136522\n",
      "iteration 700 / 1500: loss 2.174792\n",
      "iteration 800 / 1500: loss 2.085738\n",
      "iteration 900 / 1500: loss 2.107199\n",
      "iteration 1000 / 1500: loss 2.126449\n",
      "iteration 1100 / 1500: loss 2.142271\n",
      "iteration 1200 / 1500: loss 2.148749\n",
      "iteration 1300 / 1500: loss 2.110665\n",
      "iteration 1400 / 1500: loss 2.111681\n",
      "iteration 0 / 1500: loss 470.354794\n",
      "iteration 100 / 1500: loss 26.097275\n",
      "iteration 200 / 1500: loss 3.262598\n",
      "iteration 300 / 1500: loss 2.137639\n",
      "iteration 400 / 1500: loss 2.036700\n",
      "iteration 500 / 1500: loss 2.119227\n",
      "iteration 600 / 1500: loss 2.086207\n",
      "iteration 700 / 1500: loss 2.031961\n",
      "iteration 800 / 1500: loss 1.952520\n",
      "iteration 900 / 1500: loss 2.057238\n",
      "iteration 1000 / 1500: loss 2.088370\n",
      "iteration 1100 / 1500: loss 2.063350\n",
      "iteration 1200 / 1500: loss 2.043453\n",
      "iteration 1300 / 1500: loss 2.020616\n",
      "iteration 1400 / 1500: loss 1.991751\n",
      "iteration 0 / 1500: loss 559.501608\n",
      "iteration 100 / 1500: loss 18.523250\n",
      "iteration 200 / 1500: loss 2.478435\n",
      "iteration 300 / 1500: loss 2.150219\n",
      "iteration 400 / 1500: loss 2.102557\n",
      "iteration 500 / 1500: loss 2.021145\n",
      "iteration 600 / 1500: loss 2.080256\n",
      "iteration 700 / 1500: loss 2.071862\n",
      "iteration 800 / 1500: loss 2.034569\n",
      "iteration 900 / 1500: loss 1.978428\n",
      "iteration 1000 / 1500: loss 2.081931\n",
      "iteration 1100 / 1500: loss 2.099706\n",
      "iteration 1200 / 1500: loss 2.036787\n",
      "iteration 1300 / 1500: loss 2.064758\n",
      "iteration 1400 / 1500: loss 2.078307\n",
      "iteration 0 / 1500: loss 634.755284\n",
      "iteration 100 / 1500: loss 12.792922\n",
      "iteration 200 / 1500: loss 2.288672\n",
      "iteration 300 / 1500: loss 2.073343\n",
      "iteration 400 / 1500: loss 2.037305\n",
      "iteration 500 / 1500: loss 2.038921\n",
      "iteration 600 / 1500: loss 2.071574\n",
      "iteration 700 / 1500: loss 2.094474\n",
      "iteration 800 / 1500: loss 2.027029\n",
      "iteration 900 / 1500: loss 2.046368\n",
      "iteration 1000 / 1500: loss 2.070296\n",
      "iteration 1100 / 1500: loss 2.108069\n",
      "iteration 1200 / 1500: loss 2.060853\n",
      "iteration 1300 / 1500: loss 2.136985\n",
      "iteration 1400 / 1500: loss 2.071599\n",
      "iteration 0 / 1500: loss 728.961049\n",
      "iteration 100 / 1500: loss 9.123972\n",
      "iteration 200 / 1500: loss 2.166182\n",
      "iteration 300 / 1500: loss 2.089785\n",
      "iteration 400 / 1500: loss 2.153832\n",
      "iteration 500 / 1500: loss 2.043142\n",
      "iteration 600 / 1500: loss 2.025890\n",
      "iteration 700 / 1500: loss 2.073595\n",
      "iteration 800 / 1500: loss 2.068082\n",
      "iteration 900 / 1500: loss 2.107864\n",
      "iteration 1000 / 1500: loss 2.091074\n",
      "iteration 1100 / 1500: loss 2.018268\n",
      "iteration 1200 / 1500: loss 2.067139\n",
      "iteration 1300 / 1500: loss 2.035774\n",
      "iteration 1400 / 1500: loss 2.117882\n",
      "iteration 0 / 1500: loss 810.456402\n",
      "iteration 100 / 1500: loss 6.579096\n",
      "iteration 200 / 1500: loss 2.115090\n",
      "iteration 300 / 1500: loss 2.121486\n",
      "iteration 400 / 1500: loss 2.141285\n",
      "iteration 500 / 1500: loss 2.084815\n",
      "iteration 600 / 1500: loss 2.145284\n",
      "iteration 700 / 1500: loss 2.026514\n",
      "iteration 800 / 1500: loss 2.131659\n",
      "iteration 900 / 1500: loss 2.033852\n",
      "iteration 1000 / 1500: loss 2.189873\n",
      "iteration 1100 / 1500: loss 2.067375\n",
      "iteration 1200 / 1500: loss 2.169321\n",
      "iteration 1300 / 1500: loss 2.067409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1400 / 1500: loss 2.055519\n",
      "iteration 0 / 1500: loss 894.486644\n",
      "iteration 100 / 1500: loss 4.975341\n",
      "iteration 200 / 1500: loss 2.108216\n",
      "iteration 300 / 1500: loss 2.093042\n",
      "iteration 400 / 1500: loss 2.180978\n",
      "iteration 500 / 1500: loss 2.144893\n",
      "iteration 600 / 1500: loss 2.036163\n",
      "iteration 700 / 1500: loss 2.088981\n",
      "iteration 800 / 1500: loss 2.150793\n",
      "iteration 900 / 1500: loss 2.112445\n",
      "iteration 1000 / 1500: loss 2.127795\n",
      "iteration 1100 / 1500: loss 2.110780\n",
      "iteration 1200 / 1500: loss 2.137803\n",
      "iteration 1300 / 1500: loss 2.099983\n",
      "iteration 1400 / 1500: loss 2.047363\n",
      "iteration 0 / 1500: loss 986.558341\n",
      "iteration 100 / 1500: loss 3.934446\n",
      "iteration 200 / 1500: loss 2.087829\n",
      "iteration 300 / 1500: loss 2.183311\n",
      "iteration 400 / 1500: loss 2.072661\n",
      "iteration 500 / 1500: loss 2.142291\n",
      "iteration 600 / 1500: loss 2.066415\n",
      "iteration 700 / 1500: loss 2.103836\n",
      "iteration 800 / 1500: loss 2.124405\n",
      "iteration 900 / 1500: loss 2.091375\n",
      "iteration 1000 / 1500: loss 2.142421\n",
      "iteration 1100 / 1500: loss 2.099966\n",
      "iteration 1200 / 1500: loss 2.042697\n",
      "iteration 1300 / 1500: loss 2.070169\n",
      "iteration 1400 / 1500: loss 2.067503\n",
      "iteration 0 / 1500: loss 1064.759469\n",
      "iteration 100 / 1500: loss 3.241849\n",
      "iteration 200 / 1500: loss 2.151433\n",
      "iteration 300 / 1500: loss 2.175917\n",
      "iteration 400 / 1500: loss 2.129430\n",
      "iteration 500 / 1500: loss 2.127096\n",
      "iteration 600 / 1500: loss 2.156649\n",
      "iteration 700 / 1500: loss 2.115737\n",
      "iteration 800 / 1500: loss 2.133424\n",
      "iteration 900 / 1500: loss 2.153045\n",
      "iteration 1000 / 1500: loss 2.092976\n",
      "iteration 1100 / 1500: loss 2.106639\n",
      "iteration 1200 / 1500: loss 2.084222\n",
      "iteration 1300 / 1500: loss 2.102945\n",
      "iteration 1400 / 1500: loss 2.096861\n",
      "iteration 0 / 1500: loss 1139.026068\n",
      "iteration 100 / 1500: loss 2.789742\n",
      "iteration 200 / 1500: loss 2.103307\n",
      "iteration 300 / 1500: loss 2.110024\n",
      "iteration 400 / 1500: loss 2.155524\n",
      "iteration 500 / 1500: loss 2.135736\n",
      "iteration 600 / 1500: loss 2.134397\n",
      "iteration 700 / 1500: loss 2.132551\n",
      "iteration 800 / 1500: loss 2.130794\n",
      "iteration 900 / 1500: loss 2.071382\n",
      "iteration 1000 / 1500: loss 2.167265\n",
      "iteration 1100 / 1500: loss 2.166259\n",
      "iteration 1200 / 1500: loss 2.135634\n",
      "iteration 1300 / 1500: loss 2.083978\n",
      "iteration 1400 / 1500: loss 2.152584\n",
      "iteration 0 / 1500: loss 1231.298877\n",
      "iteration 100 / 1500: loss 2.547612\n",
      "iteration 200 / 1500: loss 2.169152\n",
      "iteration 300 / 1500: loss 2.111401\n",
      "iteration 400 / 1500: loss 2.159172\n",
      "iteration 500 / 1500: loss 2.109042\n",
      "iteration 600 / 1500: loss 2.122747\n",
      "iteration 700 / 1500: loss 2.130441\n",
      "iteration 800 / 1500: loss 2.134487\n",
      "iteration 900 / 1500: loss 2.108888\n",
      "iteration 1000 / 1500: loss 2.137583\n",
      "iteration 1100 / 1500: loss 2.109839\n",
      "iteration 1200 / 1500: loss 2.053532\n",
      "iteration 1300 / 1500: loss 2.175289\n",
      "iteration 1400 / 1500: loss 2.131541\n",
      "iteration 0 / 1500: loss 472.543395\n",
      "iteration 100 / 1500: loss 14.966085\n",
      "iteration 200 / 1500: loss 2.398877\n",
      "iteration 300 / 1500: loss 2.026123\n",
      "iteration 400 / 1500: loss 2.031774\n",
      "iteration 500 / 1500: loss 2.053824\n",
      "iteration 600 / 1500: loss 2.058594\n",
      "iteration 700 / 1500: loss 2.043284\n",
      "iteration 800 / 1500: loss 2.034948\n",
      "iteration 900 / 1500: loss 1.964046\n",
      "iteration 1000 / 1500: loss 2.058231\n",
      "iteration 1100 / 1500: loss 1.997258\n",
      "iteration 1200 / 1500: loss 2.070701\n",
      "iteration 1300 / 1500: loss 2.043830\n",
      "iteration 1400 / 1500: loss 2.048102\n",
      "iteration 0 / 1500: loss 549.596235\n",
      "iteration 100 / 1500: loss 9.811628\n",
      "iteration 200 / 1500: loss 2.147948\n",
      "iteration 300 / 1500: loss 2.097004\n",
      "iteration 400 / 1500: loss 2.004228\n",
      "iteration 500 / 1500: loss 2.069584\n",
      "iteration 600 / 1500: loss 2.057639\n",
      "iteration 700 / 1500: loss 2.077047\n",
      "iteration 800 / 1500: loss 2.145105\n",
      "iteration 900 / 1500: loss 2.039796\n",
      "iteration 1000 / 1500: loss 2.086446\n",
      "iteration 1100 / 1500: loss 1.990390\n",
      "iteration 1200 / 1500: loss 2.053035\n",
      "iteration 1300 / 1500: loss 2.015476\n",
      "iteration 1400 / 1500: loss 2.003171\n",
      "iteration 0 / 1500: loss 631.247524\n",
      "iteration 100 / 1500: loss 6.592871\n",
      "iteration 200 / 1500: loss 2.118717\n",
      "iteration 300 / 1500: loss 2.112623\n",
      "iteration 400 / 1500: loss 2.091887\n",
      "iteration 500 / 1500: loss 1.973431\n",
      "iteration 600 / 1500: loss 2.113087\n",
      "iteration 700 / 1500: loss 2.068943\n",
      "iteration 800 / 1500: loss 2.150161\n",
      "iteration 900 / 1500: loss 2.040559\n",
      "iteration 1000 / 1500: loss 2.077525\n",
      "iteration 1100 / 1500: loss 2.053386\n",
      "iteration 1200 / 1500: loss 2.051920\n",
      "iteration 1300 / 1500: loss 2.099409\n",
      "iteration 1400 / 1500: loss 2.070228\n",
      "iteration 0 / 1500: loss 721.905721\n",
      "iteration 100 / 1500: loss 4.702694\n",
      "iteration 200 / 1500: loss 2.098145\n",
      "iteration 300 / 1500: loss 2.055403\n",
      "iteration 400 / 1500: loss 2.069431\n",
      "iteration 500 / 1500: loss 2.054201\n",
      "iteration 600 / 1500: loss 2.066326\n",
      "iteration 700 / 1500: loss 2.040197\n",
      "iteration 800 / 1500: loss 2.075309\n",
      "iteration 900 / 1500: loss 2.132689\n",
      "iteration 1000 / 1500: loss 2.075635\n",
      "iteration 1100 / 1500: loss 2.128276\n",
      "iteration 1200 / 1500: loss 2.083616\n",
      "iteration 1300 / 1500: loss 2.085297\n",
      "iteration 1400 / 1500: loss 2.081638\n",
      "iteration 0 / 1500: loss 807.086432\n",
      "iteration 100 / 1500: loss 3.602589\n",
      "iteration 200 / 1500: loss 2.064727\n",
      "iteration 300 / 1500: loss 2.066757\n",
      "iteration 400 / 1500: loss 2.161149\n",
      "iteration 500 / 1500: loss 2.112810\n",
      "iteration 600 / 1500: loss 2.046946\n",
      "iteration 700 / 1500: loss 2.106004\n",
      "iteration 800 / 1500: loss 2.074399\n",
      "iteration 900 / 1500: loss 2.058606\n",
      "iteration 1000 / 1500: loss 2.109823\n",
      "iteration 1100 / 1500: loss 2.117083\n",
      "iteration 1200 / 1500: loss 2.064522\n",
      "iteration 1300 / 1500: loss 2.110526\n",
      "iteration 1400 / 1500: loss 2.136187\n",
      "iteration 0 / 1500: loss 881.997243\n",
      "iteration 100 / 1500: loss 2.865939\n",
      "iteration 200 / 1500: loss 2.099393\n",
      "iteration 300 / 1500: loss 2.133588\n",
      "iteration 400 / 1500: loss 2.102321\n",
      "iteration 500 / 1500: loss 2.071595\n",
      "iteration 600 / 1500: loss 2.100366\n",
      "iteration 700 / 1500: loss 2.085835\n",
      "iteration 800 / 1500: loss 2.175420\n",
      "iteration 900 / 1500: loss 2.084882\n",
      "iteration 1000 / 1500: loss 2.059804\n",
      "iteration 1100 / 1500: loss 2.103409\n",
      "iteration 1200 / 1500: loss 2.121930\n",
      "iteration 1300 / 1500: loss 2.110456\n",
      "iteration 1400 / 1500: loss 2.071308\n",
      "iteration 0 / 1500: loss 981.461899\n",
      "iteration 100 / 1500: loss 2.566610\n",
      "iteration 200 / 1500: loss 2.135261\n",
      "iteration 300 / 1500: loss 2.130266\n",
      "iteration 400 / 1500: loss 2.147319\n",
      "iteration 500 / 1500: loss 2.147331\n",
      "iteration 600 / 1500: loss 2.132186\n",
      "iteration 700 / 1500: loss 2.114448\n",
      "iteration 800 / 1500: loss 2.190309\n",
      "iteration 900 / 1500: loss 2.038506\n",
      "iteration 1000 / 1500: loss 2.114557\n",
      "iteration 1100 / 1500: loss 2.061074\n",
      "iteration 1200 / 1500: loss 2.115368\n",
      "iteration 1300 / 1500: loss 2.076857\n",
      "iteration 1400 / 1500: loss 2.087160\n",
      "iteration 0 / 1500: loss 1068.643029\n",
      "iteration 100 / 1500: loss 2.402419\n",
      "iteration 200 / 1500: loss 2.151826\n",
      "iteration 300 / 1500: loss 2.093278\n",
      "iteration 400 / 1500: loss 2.106104\n",
      "iteration 500 / 1500: loss 2.120589\n",
      "iteration 600 / 1500: loss 2.142718\n",
      "iteration 700 / 1500: loss 2.138441\n",
      "iteration 800 / 1500: loss 2.102089\n",
      "iteration 900 / 1500: loss 2.050639\n",
      "iteration 1000 / 1500: loss 2.165156\n",
      "iteration 1100 / 1500: loss 2.148135\n",
      "iteration 1200 / 1500: loss 2.083546\n",
      "iteration 1300 / 1500: loss 2.115831\n",
      "iteration 1400 / 1500: loss 2.168561\n",
      "iteration 0 / 1500: loss 1142.110353\n",
      "iteration 100 / 1500: loss 2.233587\n",
      "iteration 200 / 1500: loss 2.115142\n",
      "iteration 300 / 1500: loss 2.122041\n",
      "iteration 400 / 1500: loss 2.126315\n",
      "iteration 500 / 1500: loss 2.097549\n",
      "iteration 600 / 1500: loss 2.115076\n",
      "iteration 700 / 1500: loss 2.143004\n",
      "iteration 800 / 1500: loss 2.068077\n",
      "iteration 900 / 1500: loss 2.129132\n",
      "iteration 1000 / 1500: loss 2.151559\n",
      "iteration 1100 / 1500: loss 2.088708\n",
      "iteration 1200 / 1500: loss 2.114439\n",
      "iteration 1300 / 1500: loss 2.099329\n",
      "iteration 1400 / 1500: loss 2.100582\n",
      "iteration 0 / 1500: loss 1240.611650\n",
      "iteration 100 / 1500: loss 2.257224\n",
      "iteration 200 / 1500: loss 2.142894\n",
      "iteration 300 / 1500: loss 2.145502\n",
      "iteration 400 / 1500: loss 2.109090\n",
      "iteration 500 / 1500: loss 2.174177\n",
      "iteration 600 / 1500: loss 2.135544\n",
      "iteration 700 / 1500: loss 2.130582\n",
      "iteration 800 / 1500: loss 2.133300\n",
      "iteration 900 / 1500: loss 2.151190\n",
      "iteration 1000 / 1500: loss 2.184308\n",
      "iteration 1100 / 1500: loss 2.127242\n",
      "iteration 1200 / 1500: loss 2.125593\n",
      "iteration 1300 / 1500: loss 2.125140\n",
      "iteration 1400 / 1500: loss 2.138874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 1500: loss 466.105810\n",
      "iteration 100 / 1500: loss 8.870576\n",
      "iteration 200 / 1500: loss 2.184462\n",
      "iteration 300 / 1500: loss 2.056609\n",
      "iteration 400 / 1500: loss 2.069455\n",
      "iteration 500 / 1500: loss 2.044986\n",
      "iteration 600 / 1500: loss 1.983023\n",
      "iteration 700 / 1500: loss 2.050641\n",
      "iteration 800 / 1500: loss 2.082967\n",
      "iteration 900 / 1500: loss 2.060668\n",
      "iteration 1000 / 1500: loss 2.074452\n",
      "iteration 1100 / 1500: loss 2.044638\n",
      "iteration 1200 / 1500: loss 1.982562\n",
      "iteration 1300 / 1500: loss 1.996934\n",
      "iteration 1400 / 1500: loss 2.029978\n",
      "iteration 0 / 1500: loss 554.937691\n",
      "iteration 100 / 1500: loss 5.797895\n",
      "iteration 200 / 1500: loss 2.163053\n",
      "iteration 300 / 1500: loss 2.114743\n",
      "iteration 400 / 1500: loss 2.049452\n",
      "iteration 500 / 1500: loss 2.096755\n",
      "iteration 600 / 1500: loss 2.065586\n",
      "iteration 700 / 1500: loss 1.983859\n",
      "iteration 800 / 1500: loss 2.102799\n",
      "iteration 900 / 1500: loss 2.096293\n",
      "iteration 1000 / 1500: loss 2.054803\n",
      "iteration 1100 / 1500: loss 2.068833\n",
      "iteration 1200 / 1500: loss 2.058812\n",
      "iteration 1300 / 1500: loss 2.078124\n",
      "iteration 1400 / 1500: loss 2.116251\n",
      "iteration 0 / 1500: loss 636.692098\n",
      "iteration 100 / 1500: loss 3.957086\n",
      "iteration 200 / 1500: loss 2.109150\n",
      "iteration 300 / 1500: loss 2.138302\n",
      "iteration 400 / 1500: loss 2.023887\n",
      "iteration 500 / 1500: loss 2.077563\n",
      "iteration 600 / 1500: loss 2.040160\n",
      "iteration 700 / 1500: loss 2.129149\n",
      "iteration 800 / 1500: loss 2.083723\n",
      "iteration 900 / 1500: loss 2.121839\n",
      "iteration 1000 / 1500: loss 2.072442\n",
      "iteration 1100 / 1500: loss 2.086640\n",
      "iteration 1200 / 1500: loss 2.085641\n",
      "iteration 1300 / 1500: loss 2.098394\n",
      "iteration 1400 / 1500: loss 2.085959\n",
      "iteration 0 / 1500: loss 716.705081\n",
      "iteration 100 / 1500: loss 3.090659\n",
      "iteration 200 / 1500: loss 2.089429\n",
      "iteration 300 / 1500: loss 2.138248\n",
      "iteration 400 / 1500: loss 2.050337\n",
      "iteration 500 / 1500: loss 2.016383\n",
      "iteration 600 / 1500: loss 2.147378\n",
      "iteration 700 / 1500: loss 2.085735\n",
      "iteration 800 / 1500: loss 2.135524\n",
      "iteration 900 / 1500: loss 2.108157\n",
      "iteration 1000 / 1500: loss 2.068910\n",
      "iteration 1100 / 1500: loss 2.101350\n",
      "iteration 1200 / 1500: loss 2.072654\n",
      "iteration 1300 / 1500: loss 2.034145\n",
      "iteration 1400 / 1500: loss 2.078297\n",
      "iteration 0 / 1500: loss 814.167312\n",
      "iteration 100 / 1500: loss 2.585511\n",
      "iteration 200 / 1500: loss 2.163770\n",
      "iteration 300 / 1500: loss 2.101267\n",
      "iteration 400 / 1500: loss 2.084453\n",
      "iteration 500 / 1500: loss 2.073999\n",
      "iteration 600 / 1500: loss 2.109921\n",
      "iteration 700 / 1500: loss 2.156321\n",
      "iteration 800 / 1500: loss 2.093714\n",
      "iteration 900 / 1500: loss 2.045200\n",
      "iteration 1000 / 1500: loss 2.152159\n",
      "iteration 1100 / 1500: loss 2.085583\n",
      "iteration 1200 / 1500: loss 2.040063\n",
      "iteration 1300 / 1500: loss 2.034567\n",
      "iteration 1400 / 1500: loss 2.050659\n",
      "iteration 0 / 1500: loss 902.792516\n",
      "iteration 100 / 1500: loss 2.404453\n",
      "iteration 200 / 1500: loss 2.081397\n",
      "iteration 300 / 1500: loss 2.048022\n",
      "iteration 400 / 1500: loss 2.123063\n",
      "iteration 500 / 1500: loss 2.050313\n",
      "iteration 600 / 1500: loss 2.108795\n",
      "iteration 700 / 1500: loss 2.113340\n",
      "iteration 800 / 1500: loss 2.147521\n",
      "iteration 900 / 1500: loss 2.131647\n",
      "iteration 1000 / 1500: loss 2.113658\n",
      "iteration 1100 / 1500: loss 2.150340\n",
      "iteration 1200 / 1500: loss 2.035591\n",
      "iteration 1300 / 1500: loss 2.122850\n",
      "iteration 1400 / 1500: loss 2.141562\n",
      "iteration 0 / 1500: loss 967.997922\n",
      "iteration 100 / 1500: loss 2.163913\n",
      "iteration 200 / 1500: loss 2.077664\n",
      "iteration 300 / 1500: loss 2.077956\n",
      "iteration 400 / 1500: loss 2.148088\n",
      "iteration 500 / 1500: loss 2.144002\n",
      "iteration 600 / 1500: loss 2.067302\n",
      "iteration 700 / 1500: loss 2.075572\n",
      "iteration 800 / 1500: loss 2.108994\n",
      "iteration 900 / 1500: loss 2.106906\n",
      "iteration 1000 / 1500: loss 2.137693\n",
      "iteration 1100 / 1500: loss 2.106768\n",
      "iteration 1200 / 1500: loss 2.091516\n",
      "iteration 1300 / 1500: loss 2.075396\n",
      "iteration 1400 / 1500: loss 2.146994\n",
      "iteration 0 / 1500: loss 1056.927835\n",
      "iteration 100 / 1500: loss 2.119268\n",
      "iteration 200 / 1500: loss 2.172836\n",
      "iteration 300 / 1500: loss 2.132397\n",
      "iteration 400 / 1500: loss 2.138547\n",
      "iteration 500 / 1500: loss 2.059640\n",
      "iteration 600 / 1500: loss 2.150276\n",
      "iteration 700 / 1500: loss 2.119207\n",
      "iteration 800 / 1500: loss 2.095907\n",
      "iteration 900 / 1500: loss 2.082741\n",
      "iteration 1000 / 1500: loss 2.043232\n",
      "iteration 1100 / 1500: loss 2.114741\n",
      "iteration 1200 / 1500: loss 2.108964\n",
      "iteration 1300 / 1500: loss 2.152511\n",
      "iteration 1400 / 1500: loss 2.155311\n",
      "iteration 0 / 1500: loss 1138.313579\n",
      "iteration 100 / 1500: loss 2.165891\n",
      "iteration 200 / 1500: loss 2.165059\n",
      "iteration 300 / 1500: loss 2.112433\n",
      "iteration 400 / 1500: loss 2.167402\n",
      "iteration 500 / 1500: loss 2.144708\n",
      "iteration 600 / 1500: loss 2.121226\n",
      "iteration 700 / 1500: loss 2.122362\n",
      "iteration 800 / 1500: loss 2.111661\n",
      "iteration 900 / 1500: loss 2.116857\n",
      "iteration 1000 / 1500: loss 2.098448\n",
      "iteration 1100 / 1500: loss 2.098843\n",
      "iteration 1200 / 1500: loss 2.093621\n",
      "iteration 1300 / 1500: loss 2.164110\n",
      "iteration 1400 / 1500: loss 2.049766\n",
      "iteration 0 / 1500: loss 1235.025682\n",
      "iteration 100 / 1500: loss 2.172116\n",
      "iteration 200 / 1500: loss 2.063853\n",
      "iteration 300 / 1500: loss 2.176245\n",
      "iteration 400 / 1500: loss 2.158220\n",
      "iteration 500 / 1500: loss 2.108315\n",
      "iteration 600 / 1500: loss 2.118096\n",
      "iteration 700 / 1500: loss 2.145181\n",
      "iteration 800 / 1500: loss 2.092816\n",
      "iteration 900 / 1500: loss 2.120452\n",
      "iteration 1000 / 1500: loss 2.127843\n",
      "iteration 1100 / 1500: loss 2.076002\n",
      "iteration 1200 / 1500: loss 2.102984\n",
      "iteration 1300 / 1500: loss 2.136623\n",
      "iteration 1400 / 1500: loss 2.127914\n",
      "iteration 0 / 1500: loss 467.266543\n",
      "iteration 100 / 1500: loss 5.694995\n",
      "iteration 200 / 1500: loss 2.088360\n",
      "iteration 300 / 1500: loss 2.024558\n",
      "iteration 400 / 1500: loss 2.056793\n",
      "iteration 500 / 1500: loss 2.009103\n",
      "iteration 600 / 1500: loss 2.091880\n",
      "iteration 700 / 1500: loss 2.041354\n",
      "iteration 800 / 1500: loss 2.029119\n",
      "iteration 900 / 1500: loss 2.077337\n",
      "iteration 1000 / 1500: loss 2.098785\n",
      "iteration 1100 / 1500: loss 2.005964\n",
      "iteration 1200 / 1500: loss 2.087275\n",
      "iteration 1300 / 1500: loss 2.072524\n",
      "iteration 1400 / 1500: loss 2.071917\n",
      "iteration 0 / 1500: loss 553.759278\n",
      "iteration 100 / 1500: loss 3.805606\n",
      "iteration 200 / 1500: loss 2.071664\n",
      "iteration 300 / 1500: loss 2.031119\n",
      "iteration 400 / 1500: loss 2.074871\n",
      "iteration 500 / 1500: loss 2.073819\n",
      "iteration 600 / 1500: loss 2.096404\n",
      "iteration 700 / 1500: loss 2.089691\n",
      "iteration 800 / 1500: loss 2.136330\n",
      "iteration 900 / 1500: loss 2.025471\n",
      "iteration 1000 / 1500: loss 2.041067\n",
      "iteration 1100 / 1500: loss 2.100591\n",
      "iteration 1200 / 1500: loss 1.982328\n",
      "iteration 1300 / 1500: loss 2.073711\n",
      "iteration 1400 / 1500: loss 2.055383\n",
      "iteration 0 / 1500: loss 639.738322\n",
      "iteration 100 / 1500: loss 2.894104\n",
      "iteration 200 / 1500: loss 2.042771\n",
      "iteration 300 / 1500: loss 2.038379\n",
      "iteration 400 / 1500: loss 2.096334\n",
      "iteration 500 / 1500: loss 2.077909\n",
      "iteration 600 / 1500: loss 2.096564\n",
      "iteration 700 / 1500: loss 2.065873\n",
      "iteration 800 / 1500: loss 2.110165\n",
      "iteration 900 / 1500: loss 2.120116\n",
      "iteration 1000 / 1500: loss 2.085650\n",
      "iteration 1100 / 1500: loss 2.054848\n",
      "iteration 1200 / 1500: loss 2.180718\n",
      "iteration 1300 / 1500: loss 2.084374\n",
      "iteration 1400 / 1500: loss 2.048631\n",
      "iteration 0 / 1500: loss 719.921881\n",
      "iteration 100 / 1500: loss 2.501034\n",
      "iteration 200 / 1500: loss 2.093003\n",
      "iteration 300 / 1500: loss 2.049918\n",
      "iteration 400 / 1500: loss 2.124167\n",
      "iteration 500 / 1500: loss 2.092283\n",
      "iteration 600 / 1500: loss 2.130371\n",
      "iteration 700 / 1500: loss 2.168349\n",
      "iteration 800 / 1500: loss 2.146460\n",
      "iteration 900 / 1500: loss 2.118611\n",
      "iteration 1000 / 1500: loss 2.081847\n",
      "iteration 1100 / 1500: loss 2.075311\n",
      "iteration 1200 / 1500: loss 2.113794\n",
      "iteration 1300 / 1500: loss 2.107447\n",
      "iteration 1400 / 1500: loss 2.110096\n",
      "iteration 0 / 1500: loss 815.432720\n",
      "iteration 100 / 1500: loss 2.277290\n",
      "iteration 200 / 1500: loss 2.075281\n",
      "iteration 300 / 1500: loss 2.045595\n",
      "iteration 400 / 1500: loss 2.145175\n",
      "iteration 500 / 1500: loss 2.050841\n",
      "iteration 600 / 1500: loss 2.101286\n",
      "iteration 700 / 1500: loss 2.117558\n",
      "iteration 800 / 1500: loss 2.120214\n",
      "iteration 900 / 1500: loss 2.102350\n",
      "iteration 1000 / 1500: loss 2.075543\n",
      "iteration 1100 / 1500: loss 2.122979\n",
      "iteration 1200 / 1500: loss 2.056145\n",
      "iteration 1300 / 1500: loss 2.076212\n",
      "iteration 1400 / 1500: loss 2.125302\n",
      "iteration 0 / 1500: loss 891.099939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100 / 1500: loss 2.226085\n",
      "iteration 200 / 1500: loss 2.123615\n",
      "iteration 300 / 1500: loss 2.129464\n",
      "iteration 400 / 1500: loss 2.079126\n",
      "iteration 500 / 1500: loss 2.064320\n",
      "iteration 600 / 1500: loss 2.093638\n",
      "iteration 700 / 1500: loss 2.153648\n",
      "iteration 800 / 1500: loss 2.150737\n",
      "iteration 900 / 1500: loss 2.101615\n",
      "iteration 1000 / 1500: loss 2.067045\n",
      "iteration 1100 / 1500: loss 2.107193\n",
      "iteration 1200 / 1500: loss 2.141119\n",
      "iteration 1300 / 1500: loss 2.107476\n",
      "iteration 1400 / 1500: loss 2.107766\n",
      "iteration 0 / 1500: loss 975.160127\n",
      "iteration 100 / 1500: loss 2.170841\n",
      "iteration 200 / 1500: loss 2.081013\n",
      "iteration 300 / 1500: loss 2.082717\n",
      "iteration 400 / 1500: loss 2.093519\n",
      "iteration 500 / 1500: loss 2.216109\n",
      "iteration 600 / 1500: loss 2.114383\n",
      "iteration 700 / 1500: loss 2.114413\n",
      "iteration 800 / 1500: loss 2.135828\n",
      "iteration 900 / 1500: loss 2.151438\n",
      "iteration 1000 / 1500: loss 2.085303\n",
      "iteration 1100 / 1500: loss 2.120017\n",
      "iteration 1200 / 1500: loss 2.109237\n",
      "iteration 1300 / 1500: loss 2.113717\n",
      "iteration 1400 / 1500: loss 2.107258\n",
      "iteration 0 / 1500: loss 1059.491903\n",
      "iteration 100 / 1500: loss 2.126063\n",
      "iteration 200 / 1500: loss 2.122128\n",
      "iteration 300 / 1500: loss 2.093304\n",
      "iteration 400 / 1500: loss 2.125842\n",
      "iteration 500 / 1500: loss 2.095607\n",
      "iteration 600 / 1500: loss 2.108787\n",
      "iteration 700 / 1500: loss 2.088020\n",
      "iteration 800 / 1500: loss 2.152114\n",
      "iteration 900 / 1500: loss 2.092485\n",
      "iteration 1000 / 1500: loss 2.123666\n",
      "iteration 1100 / 1500: loss 2.179786\n",
      "iteration 1200 / 1500: loss 2.178171\n",
      "iteration 1300 / 1500: loss 2.087428\n",
      "iteration 1400 / 1500: loss 2.168152\n",
      "iteration 0 / 1500: loss 1162.859253\n",
      "iteration 100 / 1500: loss 2.171716\n",
      "iteration 200 / 1500: loss 2.162720\n",
      "iteration 300 / 1500: loss 2.159415\n",
      "iteration 400 / 1500: loss 2.097170\n",
      "iteration 500 / 1500: loss 2.115323\n",
      "iteration 600 / 1500: loss 2.074130\n",
      "iteration 700 / 1500: loss 2.088638\n",
      "iteration 800 / 1500: loss 2.064163\n",
      "iteration 900 / 1500: loss 2.169872\n",
      "iteration 1000 / 1500: loss 2.143507\n",
      "iteration 1100 / 1500: loss 2.123922\n",
      "iteration 1200 / 1500: loss 2.180886\n",
      "iteration 1300 / 1500: loss 2.107531\n",
      "iteration 1400 / 1500: loss 2.170842\n",
      "iteration 0 / 1500: loss 1246.716101\n",
      "iteration 100 / 1500: loss 2.143575\n",
      "iteration 200 / 1500: loss 2.118596\n",
      "iteration 300 / 1500: loss 2.191422\n",
      "iteration 400 / 1500: loss 2.079679\n",
      "iteration 500 / 1500: loss 2.147612\n",
      "iteration 600 / 1500: loss 2.123344\n",
      "iteration 700 / 1500: loss 2.131276\n",
      "iteration 800 / 1500: loss 2.133711\n",
      "iteration 900 / 1500: loss 2.154185\n",
      "iteration 1000 / 1500: loss 2.211470\n",
      "iteration 1100 / 1500: loss 2.132813\n",
      "iteration 1200 / 1500: loss 2.151343\n",
      "iteration 1300 / 1500: loss 2.124164\n",
      "iteration 1400 / 1500: loss 2.068624\n",
      "iteration 0 / 1500: loss 471.642651\n",
      "iteration 100 / 1500: loss 4.031040\n",
      "iteration 200 / 1500: loss 2.029315\n",
      "iteration 300 / 1500: loss 2.055035\n",
      "iteration 400 / 1500: loss 2.017483\n",
      "iteration 500 / 1500: loss 2.130123\n",
      "iteration 600 / 1500: loss 2.028285\n",
      "iteration 700 / 1500: loss 2.045799\n",
      "iteration 800 / 1500: loss 1.995489\n",
      "iteration 900 / 1500: loss 2.041419\n",
      "iteration 1000 / 1500: loss 2.115482\n",
      "iteration 1100 / 1500: loss 1.988181\n",
      "iteration 1200 / 1500: loss 2.041211\n",
      "iteration 1300 / 1500: loss 2.076127\n",
      "iteration 1400 / 1500: loss 2.071525\n",
      "iteration 0 / 1500: loss 556.663710\n",
      "iteration 100 / 1500: loss 2.870107\n",
      "iteration 200 / 1500: loss 2.084467\n",
      "iteration 300 / 1500: loss 2.045323\n",
      "iteration 400 / 1500: loss 2.094691\n",
      "iteration 500 / 1500: loss 2.028877\n",
      "iteration 600 / 1500: loss 2.055283\n",
      "iteration 700 / 1500: loss 2.029147\n",
      "iteration 800 / 1500: loss 2.041836\n",
      "iteration 900 / 1500: loss 2.002079\n",
      "iteration 1000 / 1500: loss 2.076038\n",
      "iteration 1100 / 1500: loss 2.087174\n",
      "iteration 1200 / 1500: loss 2.098564\n",
      "iteration 1300 / 1500: loss 2.169864\n",
      "iteration 1400 / 1500: loss 2.103360\n",
      "iteration 0 / 1500: loss 634.779614\n",
      "iteration 100 / 1500: loss 2.395575\n",
      "iteration 200 / 1500: loss 2.104136\n",
      "iteration 300 / 1500: loss 2.098565\n",
      "iteration 400 / 1500: loss 2.078249\n",
      "iteration 500 / 1500: loss 2.102406\n",
      "iteration 600 / 1500: loss 2.104306\n",
      "iteration 700 / 1500: loss 2.004028\n",
      "iteration 800 / 1500: loss 2.074071\n",
      "iteration 900 / 1500: loss 2.064732\n",
      "iteration 1000 / 1500: loss 2.106805\n",
      "iteration 1100 / 1500: loss 2.107380\n",
      "iteration 1200 / 1500: loss 2.147017\n",
      "iteration 1300 / 1500: loss 2.184200\n",
      "iteration 1400 / 1500: loss 2.121810\n",
      "iteration 0 / 1500: loss 717.993004\n",
      "iteration 100 / 1500: loss 2.248431\n",
      "iteration 200 / 1500: loss 2.092809\n",
      "iteration 300 / 1500: loss 2.093111\n",
      "iteration 400 / 1500: loss 2.101515\n",
      "iteration 500 / 1500: loss 2.055965\n",
      "iteration 600 / 1500: loss 2.087965\n",
      "iteration 700 / 1500: loss 2.023518\n",
      "iteration 800 / 1500: loss 2.064529\n",
      "iteration 900 / 1500: loss 2.051211\n",
      "iteration 1000 / 1500: loss 2.063640\n",
      "iteration 1100 / 1500: loss 2.111157\n",
      "iteration 1200 / 1500: loss 1.998179\n",
      "iteration 1300 / 1500: loss 2.101337\n",
      "iteration 1400 / 1500: loss 2.082031\n",
      "iteration 0 / 1500: loss 804.810426\n",
      "iteration 100 / 1500: loss 2.081846\n",
      "iteration 200 / 1500: loss 2.055732\n",
      "iteration 300 / 1500: loss 2.105090\n",
      "iteration 400 / 1500: loss 2.099577\n",
      "iteration 500 / 1500: loss 2.151538\n",
      "iteration 600 / 1500: loss 2.048849\n",
      "iteration 700 / 1500: loss 2.074820\n",
      "iteration 800 / 1500: loss 2.070509\n",
      "iteration 900 / 1500: loss 2.090750\n",
      "iteration 1000 / 1500: loss 2.071036\n",
      "iteration 1100 / 1500: loss 2.095904\n",
      "iteration 1200 / 1500: loss 2.059259\n",
      "iteration 1300 / 1500: loss 2.080722\n",
      "iteration 1400 / 1500: loss 2.111693\n",
      "iteration 0 / 1500: loss 892.538776\n",
      "iteration 100 / 1500: loss 2.050543\n",
      "iteration 200 / 1500: loss 2.122545\n",
      "iteration 300 / 1500: loss 2.021466\n",
      "iteration 400 / 1500: loss 2.116574\n",
      "iteration 500 / 1500: loss 2.133235\n",
      "iteration 600 / 1500: loss 2.185287\n",
      "iteration 700 / 1500: loss 2.129973\n",
      "iteration 800 / 1500: loss 2.049527\n",
      "iteration 900 / 1500: loss 2.168705\n",
      "iteration 1000 / 1500: loss 2.132791\n",
      "iteration 1100 / 1500: loss 2.101919\n",
      "iteration 1200 / 1500: loss 2.096535\n",
      "iteration 1300 / 1500: loss 2.130244\n",
      "iteration 1400 / 1500: loss 2.152279\n",
      "iteration 0 / 1500: loss 978.421020\n",
      "iteration 100 / 1500: loss 2.105735\n",
      "iteration 200 / 1500: loss 2.092988\n",
      "iteration 300 / 1500: loss 2.146775\n",
      "iteration 400 / 1500: loss 2.123716\n",
      "iteration 500 / 1500: loss 2.124691\n",
      "iteration 600 / 1500: loss 2.057117\n",
      "iteration 700 / 1500: loss 2.104821\n",
      "iteration 800 / 1500: loss 2.129328\n",
      "iteration 900 / 1500: loss 2.132307\n",
      "iteration 1000 / 1500: loss 2.104280\n",
      "iteration 1100 / 1500: loss 2.092184\n",
      "iteration 1200 / 1500: loss 2.046483\n",
      "iteration 1300 / 1500: loss 2.156946\n",
      "iteration 1400 / 1500: loss 2.125359\n",
      "iteration 0 / 1500: loss 1061.267737\n",
      "iteration 100 / 1500: loss 2.150430\n",
      "iteration 200 / 1500: loss 2.172350\n",
      "iteration 300 / 1500: loss 2.148304\n",
      "iteration 400 / 1500: loss 2.136129\n",
      "iteration 500 / 1500: loss 2.141354\n",
      "iteration 600 / 1500: loss 2.183301\n",
      "iteration 700 / 1500: loss 2.141115\n",
      "iteration 800 / 1500: loss 2.138731\n",
      "iteration 900 / 1500: loss 2.115277\n",
      "iteration 1000 / 1500: loss 2.093788\n",
      "iteration 1100 / 1500: loss 2.098716\n",
      "iteration 1200 / 1500: loss 2.130133\n",
      "iteration 1300 / 1500: loss 2.134812\n",
      "iteration 1400 / 1500: loss 2.093540\n",
      "iteration 0 / 1500: loss 1153.714607\n",
      "iteration 100 / 1500: loss 2.076982\n",
      "iteration 200 / 1500: loss 2.190846\n",
      "iteration 300 / 1500: loss 2.143713\n",
      "iteration 400 / 1500: loss 2.106288\n",
      "iteration 500 / 1500: loss 2.105102\n",
      "iteration 600 / 1500: loss 2.134681\n",
      "iteration 700 / 1500: loss 2.064435\n",
      "iteration 800 / 1500: loss 2.129468\n",
      "iteration 900 / 1500: loss 2.170095\n",
      "iteration 1000 / 1500: loss 2.132957\n",
      "iteration 1100 / 1500: loss 2.139649\n",
      "iteration 1200 / 1500: loss 2.172389\n",
      "iteration 1300 / 1500: loss 2.100929\n",
      "iteration 1400 / 1500: loss 2.158942\n",
      "iteration 0 / 1500: loss 1231.235252\n",
      "iteration 100 / 1500: loss 2.093265\n",
      "iteration 200 / 1500: loss 2.078359\n",
      "iteration 300 / 1500: loss 2.168973\n",
      "iteration 400 / 1500: loss 2.141261\n",
      "iteration 500 / 1500: loss 2.139326\n",
      "iteration 600 / 1500: loss 2.162487\n",
      "iteration 700 / 1500: loss 2.077057\n",
      "iteration 800 / 1500: loss 2.150929\n",
      "iteration 900 / 1500: loss 2.113929\n",
      "iteration 1000 / 1500: loss 2.158333\n",
      "iteration 1100 / 1500: loss 2.123516\n",
      "iteration 1200 / 1500: loss 2.147544\n",
      "iteration 1300 / 1500: loss 2.090607\n",
      "iteration 1400 / 1500: loss 2.150859\n",
      "iteration 0 / 1500: loss 467.122126\n",
      "iteration 100 / 1500: loss 3.146612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 200 / 1500: loss 2.047183\n",
      "iteration 300 / 1500: loss 2.126848\n",
      "iteration 400 / 1500: loss 2.126112\n",
      "iteration 500 / 1500: loss 2.108552\n",
      "iteration 600 / 1500: loss 2.032445\n",
      "iteration 700 / 1500: loss 2.019633\n",
      "iteration 800 / 1500: loss 1.997095\n",
      "iteration 900 / 1500: loss 2.040159\n",
      "iteration 1000 / 1500: loss 2.065858\n",
      "iteration 1100 / 1500: loss 2.055456\n",
      "iteration 1200 / 1500: loss 2.073238\n",
      "iteration 1300 / 1500: loss 2.012458\n",
      "iteration 1400 / 1500: loss 2.058727\n",
      "iteration 0 / 1500: loss 550.999632\n",
      "iteration 100 / 1500: loss 2.441205\n",
      "iteration 200 / 1500: loss 1.964674\n",
      "iteration 300 / 1500: loss 2.093344\n",
      "iteration 400 / 1500: loss 2.083172\n",
      "iteration 500 / 1500: loss 2.065520\n",
      "iteration 600 / 1500: loss 2.010713\n",
      "iteration 700 / 1500: loss 2.112156\n",
      "iteration 800 / 1500: loss 2.105747\n",
      "iteration 900 / 1500: loss 2.063571\n",
      "iteration 1000 / 1500: loss 2.097460\n",
      "iteration 1100 / 1500: loss 2.096861\n",
      "iteration 1200 / 1500: loss 2.057498\n",
      "iteration 1300 / 1500: loss 2.067781\n",
      "iteration 1400 / 1500: loss 2.037256\n",
      "iteration 0 / 1500: loss 640.710956\n",
      "iteration 100 / 1500: loss 2.238881\n",
      "iteration 200 / 1500: loss 2.021092\n",
      "iteration 300 / 1500: loss 2.098815\n",
      "iteration 400 / 1500: loss 2.068379\n",
      "iteration 500 / 1500: loss 2.071556\n",
      "iteration 600 / 1500: loss 2.045587\n",
      "iteration 700 / 1500: loss 2.056877\n",
      "iteration 800 / 1500: loss 2.079690\n",
      "iteration 900 / 1500: loss 2.082544\n",
      "iteration 1000 / 1500: loss 2.101063\n",
      "iteration 1100 / 1500: loss 2.109228\n",
      "iteration 1200 / 1500: loss 2.099474\n",
      "iteration 1300 / 1500: loss 2.077549\n",
      "iteration 1400 / 1500: loss 2.094232\n",
      "iteration 0 / 1500: loss 723.761099\n",
      "iteration 100 / 1500: loss 2.102509\n",
      "iteration 200 / 1500: loss 2.069982\n",
      "iteration 300 / 1500: loss 2.048351\n",
      "iteration 400 / 1500: loss 2.118834\n",
      "iteration 500 / 1500: loss 2.069880\n",
      "iteration 600 / 1500: loss 2.061332\n",
      "iteration 700 / 1500: loss 2.134717\n",
      "iteration 800 / 1500: loss 2.077454\n",
      "iteration 900 / 1500: loss 2.066949\n",
      "iteration 1000 / 1500: loss 2.118859\n",
      "iteration 1100 / 1500: loss 2.054138\n",
      "iteration 1200 / 1500: loss 2.062460\n",
      "iteration 1300 / 1500: loss 2.083230\n",
      "iteration 1400 / 1500: loss 2.077583\n",
      "iteration 0 / 1500: loss 814.289697\n",
      "iteration 100 / 1500: loss 2.144217\n",
      "iteration 200 / 1500: loss 2.117028\n",
      "iteration 300 / 1500: loss 2.071299\n",
      "iteration 400 / 1500: loss 2.090707\n",
      "iteration 500 / 1500: loss 2.082095\n",
      "iteration 600 / 1500: loss 2.119077\n",
      "iteration 700 / 1500: loss 2.131515\n",
      "iteration 800 / 1500: loss 2.139522\n",
      "iteration 900 / 1500: loss 2.071935\n",
      "iteration 1000 / 1500: loss 1.995802\n",
      "iteration 1100 / 1500: loss 2.080709\n",
      "iteration 1200 / 1500: loss 2.069053\n",
      "iteration 1300 / 1500: loss 2.158015\n",
      "iteration 1400 / 1500: loss 2.013248\n",
      "iteration 0 / 1500: loss 882.651028\n",
      "iteration 100 / 1500: loss 2.087761\n",
      "iteration 200 / 1500: loss 2.190231\n",
      "iteration 300 / 1500: loss 2.049567\n",
      "iteration 400 / 1500: loss 2.118111\n",
      "iteration 500 / 1500: loss 2.105311\n",
      "iteration 600 / 1500: loss 2.115002\n",
      "iteration 700 / 1500: loss 2.098542\n",
      "iteration 800 / 1500: loss 2.071331\n",
      "iteration 900 / 1500: loss 2.159080\n",
      "iteration 1000 / 1500: loss 2.090800\n",
      "iteration 1100 / 1500: loss 2.115137\n",
      "iteration 1200 / 1500: loss 2.078494\n",
      "iteration 1300 / 1500: loss 2.162096\n",
      "iteration 1400 / 1500: loss 2.068033\n",
      "iteration 0 / 1500: loss 982.381256\n",
      "iteration 100 / 1500: loss 2.070057\n",
      "iteration 200 / 1500: loss 2.128213\n",
      "iteration 300 / 1500: loss 2.126240\n",
      "iteration 400 / 1500: loss 2.106075\n",
      "iteration 500 / 1500: loss 2.145987\n",
      "iteration 600 / 1500: loss 2.077072\n",
      "iteration 700 / 1500: loss 2.100437\n",
      "iteration 800 / 1500: loss 2.139017\n",
      "iteration 900 / 1500: loss 2.100861\n",
      "iteration 1000 / 1500: loss 2.104368\n",
      "iteration 1100 / 1500: loss 2.126603\n",
      "iteration 1200 / 1500: loss 2.147854\n",
      "iteration 1300 / 1500: loss 2.092017\n",
      "iteration 1400 / 1500: loss 2.090756\n",
      "iteration 0 / 1500: loss 1061.798875\n",
      "iteration 100 / 1500: loss 2.130780\n",
      "iteration 200 / 1500: loss 2.086699\n",
      "iteration 300 / 1500: loss 2.167264\n",
      "iteration 400 / 1500: loss 2.105385\n",
      "iteration 500 / 1500: loss 2.157068\n",
      "iteration 600 / 1500: loss 2.211300\n",
      "iteration 700 / 1500: loss 2.129535\n",
      "iteration 800 / 1500: loss 2.143764\n",
      "iteration 900 / 1500: loss 2.128213\n",
      "iteration 1000 / 1500: loss 2.144176\n",
      "iteration 1100 / 1500: loss 2.152060\n",
      "iteration 1200 / 1500: loss 2.085546\n",
      "iteration 1300 / 1500: loss 2.117069\n",
      "iteration 1400 / 1500: loss 2.119684\n",
      "iteration 0 / 1500: loss 1141.771078\n",
      "iteration 100 / 1500: loss 2.142591\n",
      "iteration 200 / 1500: loss 2.186089\n",
      "iteration 300 / 1500: loss 2.117621\n",
      "iteration 400 / 1500: loss 2.191261\n",
      "iteration 500 / 1500: loss 2.074274\n",
      "iteration 600 / 1500: loss 2.137498\n",
      "iteration 700 / 1500: loss 2.092279\n",
      "iteration 800 / 1500: loss 2.111192\n",
      "iteration 900 / 1500: loss 2.093665\n",
      "iteration 1000 / 1500: loss 2.161012\n",
      "iteration 1100 / 1500: loss 2.113945\n",
      "iteration 1200 / 1500: loss 2.143529\n",
      "iteration 1300 / 1500: loss 2.082231\n",
      "iteration 1400 / 1500: loss 2.146085\n",
      "iteration 0 / 1500: loss 1239.178520\n",
      "iteration 100 / 1500: loss 2.087490\n",
      "iteration 200 / 1500: loss 2.107187\n",
      "iteration 300 / 1500: loss 2.101655\n",
      "iteration 400 / 1500: loss 2.126228\n",
      "iteration 500 / 1500: loss 2.143764\n",
      "iteration 600 / 1500: loss 2.121496\n",
      "iteration 700 / 1500: loss 2.081205\n",
      "iteration 800 / 1500: loss 2.181074\n",
      "iteration 900 / 1500: loss 2.112314\n",
      "iteration 1000 / 1500: loss 2.139659\n",
      "iteration 1100 / 1500: loss 2.107832\n",
      "iteration 1200 / 1500: loss 2.149560\n",
      "iteration 1300 / 1500: loss 2.094742\n",
      "iteration 1400 / 1500: loss 2.103958\n",
      "lr 8.000000e-08 reg 1.500000e+04 train accuracy: 0.344347 val accuracy: 0.355000\n",
      "lr 8.000000e-08 reg 1.777778e+04 train accuracy: 0.341837 val accuracy: 0.355000\n",
      "lr 8.000000e-08 reg 2.055556e+04 train accuracy: 0.333694 val accuracy: 0.344000\n",
      "lr 8.000000e-08 reg 2.333333e+04 train accuracy: 0.330857 val accuracy: 0.347000\n",
      "lr 8.000000e-08 reg 2.611111e+04 train accuracy: 0.326306 val accuracy: 0.337000\n",
      "lr 8.000000e-08 reg 2.888889e+04 train accuracy: 0.325408 val accuracy: 0.352000\n",
      "lr 8.000000e-08 reg 3.166667e+04 train accuracy: 0.317204 val accuracy: 0.334000\n",
      "lr 8.000000e-08 reg 3.444444e+04 train accuracy: 0.322102 val accuracy: 0.344000\n",
      "lr 8.000000e-08 reg 3.722222e+04 train accuracy: 0.315571 val accuracy: 0.333000\n",
      "lr 8.000000e-08 reg 4.000000e+04 train accuracy: 0.314061 val accuracy: 0.336000\n",
      "lr 1.822222e-07 reg 1.500000e+04 train accuracy: 0.346082 val accuracy: 0.359000\n",
      "lr 1.822222e-07 reg 1.777778e+04 train accuracy: 0.339429 val accuracy: 0.351000\n",
      "lr 1.822222e-07 reg 2.055556e+04 train accuracy: 0.333633 val accuracy: 0.356000\n",
      "lr 1.822222e-07 reg 2.333333e+04 train accuracy: 0.334102 val accuracy: 0.346000\n",
      "lr 1.822222e-07 reg 2.611111e+04 train accuracy: 0.316388 val accuracy: 0.329000\n",
      "lr 1.822222e-07 reg 2.888889e+04 train accuracy: 0.327755 val accuracy: 0.344000\n",
      "lr 1.822222e-07 reg 3.166667e+04 train accuracy: 0.322714 val accuracy: 0.338000\n",
      "lr 1.822222e-07 reg 3.444444e+04 train accuracy: 0.324878 val accuracy: 0.344000\n",
      "lr 1.822222e-07 reg 3.722222e+04 train accuracy: 0.312306 val accuracy: 0.320000\n",
      "lr 1.822222e-07 reg 4.000000e+04 train accuracy: 0.305714 val accuracy: 0.333000\n",
      "lr 2.844444e-07 reg 1.500000e+04 train accuracy: 0.347041 val accuracy: 0.359000\n",
      "lr 2.844444e-07 reg 1.777778e+04 train accuracy: 0.338776 val accuracy: 0.357000\n",
      "lr 2.844444e-07 reg 2.055556e+04 train accuracy: 0.338796 val accuracy: 0.354000\n",
      "lr 2.844444e-07 reg 2.333333e+04 train accuracy: 0.334735 val accuracy: 0.339000\n",
      "lr 2.844444e-07 reg 2.611111e+04 train accuracy: 0.322245 val accuracy: 0.336000\n",
      "lr 2.844444e-07 reg 2.888889e+04 train accuracy: 0.327102 val accuracy: 0.345000\n",
      "lr 2.844444e-07 reg 3.166667e+04 train accuracy: 0.318531 val accuracy: 0.329000\n",
      "lr 2.844444e-07 reg 3.444444e+04 train accuracy: 0.323571 val accuracy: 0.337000\n",
      "lr 2.844444e-07 reg 3.722222e+04 train accuracy: 0.312959 val accuracy: 0.328000\n",
      "lr 2.844444e-07 reg 4.000000e+04 train accuracy: 0.316408 val accuracy: 0.328000\n",
      "lr 3.866667e-07 reg 1.500000e+04 train accuracy: 0.348714 val accuracy: 0.355000\n",
      "lr 3.866667e-07 reg 1.777778e+04 train accuracy: 0.337000 val accuracy: 0.334000\n",
      "lr 3.866667e-07 reg 2.055556e+04 train accuracy: 0.331980 val accuracy: 0.351000\n",
      "lr 3.866667e-07 reg 2.333333e+04 train accuracy: 0.338306 val accuracy: 0.341000\n",
      "lr 3.866667e-07 reg 2.611111e+04 train accuracy: 0.331776 val accuracy: 0.350000\n",
      "lr 3.866667e-07 reg 2.888889e+04 train accuracy: 0.314388 val accuracy: 0.329000\n",
      "lr 3.866667e-07 reg 3.166667e+04 train accuracy: 0.316959 val accuracy: 0.338000\n",
      "lr 3.866667e-07 reg 3.444444e+04 train accuracy: 0.310204 val accuracy: 0.332000\n",
      "lr 3.866667e-07 reg 3.722222e+04 train accuracy: 0.315367 val accuracy: 0.331000\n",
      "lr 3.866667e-07 reg 4.000000e+04 train accuracy: 0.304061 val accuracy: 0.318000\n",
      "lr 4.888889e-07 reg 1.500000e+04 train accuracy: 0.349449 val accuracy: 0.356000\n",
      "lr 4.888889e-07 reg 1.777778e+04 train accuracy: 0.334000 val accuracy: 0.344000\n",
      "lr 4.888889e-07 reg 2.055556e+04 train accuracy: 0.336837 val accuracy: 0.334000\n",
      "lr 4.888889e-07 reg 2.333333e+04 train accuracy: 0.322551 val accuracy: 0.337000\n",
      "lr 4.888889e-07 reg 2.611111e+04 train accuracy: 0.326918 val accuracy: 0.337000\n",
      "lr 4.888889e-07 reg 2.888889e+04 train accuracy: 0.310245 val accuracy: 0.321000\n",
      "lr 4.888889e-07 reg 3.166667e+04 train accuracy: 0.316694 val accuracy: 0.314000\n",
      "lr 4.888889e-07 reg 3.444444e+04 train accuracy: 0.317633 val accuracy: 0.328000\n",
      "lr 4.888889e-07 reg 3.722222e+04 train accuracy: 0.297673 val accuracy: 0.317000\n",
      "lr 4.888889e-07 reg 4.000000e+04 train accuracy: 0.308224 val accuracy: 0.327000\n",
      "lr 5.911111e-07 reg 1.500000e+04 train accuracy: 0.337510 val accuracy: 0.334000\n",
      "lr 5.911111e-07 reg 1.777778e+04 train accuracy: 0.333980 val accuracy: 0.344000\n",
      "lr 5.911111e-07 reg 2.055556e+04 train accuracy: 0.324265 val accuracy: 0.344000\n",
      "lr 5.911111e-07 reg 2.333333e+04 train accuracy: 0.324000 val accuracy: 0.340000\n",
      "lr 5.911111e-07 reg 2.611111e+04 train accuracy: 0.317612 val accuracy: 0.325000\n",
      "lr 5.911111e-07 reg 2.888889e+04 train accuracy: 0.314918 val accuracy: 0.324000\n",
      "lr 5.911111e-07 reg 3.166667e+04 train accuracy: 0.323490 val accuracy: 0.323000\n",
      "lr 5.911111e-07 reg 3.444444e+04 train accuracy: 0.310714 val accuracy: 0.320000\n",
      "lr 5.911111e-07 reg 3.722222e+04 train accuracy: 0.318531 val accuracy: 0.323000\n",
      "lr 5.911111e-07 reg 4.000000e+04 train accuracy: 0.312327 val accuracy: 0.319000\n",
      "lr 6.933333e-07 reg 1.500000e+04 train accuracy: 0.342327 val accuracy: 0.354000\n",
      "lr 6.933333e-07 reg 1.777778e+04 train accuracy: 0.335102 val accuracy: 0.353000\n",
      "lr 6.933333e-07 reg 2.055556e+04 train accuracy: 0.329612 val accuracy: 0.345000\n",
      "lr 6.933333e-07 reg 2.333333e+04 train accuracy: 0.312714 val accuracy: 0.343000\n",
      "lr 6.933333e-07 reg 2.611111e+04 train accuracy: 0.312306 val accuracy: 0.310000\n",
      "lr 6.933333e-07 reg 2.888889e+04 train accuracy: 0.308408 val accuracy: 0.323000\n",
      "lr 6.933333e-07 reg 3.166667e+04 train accuracy: 0.316735 val accuracy: 0.324000\n",
      "lr 6.933333e-07 reg 3.444444e+04 train accuracy: 0.311673 val accuracy: 0.321000\n",
      "lr 6.933333e-07 reg 3.722222e+04 train accuracy: 0.308776 val accuracy: 0.311000\n",
      "lr 6.933333e-07 reg 4.000000e+04 train accuracy: 0.297388 val accuracy: 0.309000\n",
      "lr 7.955556e-07 reg 1.500000e+04 train accuracy: 0.348020 val accuracy: 0.357000\n",
      "lr 7.955556e-07 reg 1.777778e+04 train accuracy: 0.341776 val accuracy: 0.344000\n",
      "lr 7.955556e-07 reg 2.055556e+04 train accuracy: 0.332816 val accuracy: 0.333000\n",
      "lr 7.955556e-07 reg 2.333333e+04 train accuracy: 0.329939 val accuracy: 0.346000\n",
      "lr 7.955556e-07 reg 2.611111e+04 train accuracy: 0.326082 val accuracy: 0.334000\n",
      "lr 7.955556e-07 reg 2.888889e+04 train accuracy: 0.321102 val accuracy: 0.340000\n",
      "lr 7.955556e-07 reg 3.166667e+04 train accuracy: 0.309694 val accuracy: 0.320000\n",
      "lr 7.955556e-07 reg 3.444444e+04 train accuracy: 0.312592 val accuracy: 0.332000\n",
      "lr 7.955556e-07 reg 3.722222e+04 train accuracy: 0.309449 val accuracy: 0.339000\n",
      "lr 7.955556e-07 reg 4.000000e+04 train accuracy: 0.302918 val accuracy: 0.322000\n",
      "lr 8.977778e-07 reg 1.500000e+04 train accuracy: 0.337816 val accuracy: 0.337000\n",
      "lr 8.977778e-07 reg 1.777778e+04 train accuracy: 0.328776 val accuracy: 0.351000\n",
      "lr 8.977778e-07 reg 2.055556e+04 train accuracy: 0.335184 val accuracy: 0.344000\n",
      "lr 8.977778e-07 reg 2.333333e+04 train accuracy: 0.320939 val accuracy: 0.330000\n",
      "lr 8.977778e-07 reg 2.611111e+04 train accuracy: 0.317939 val accuracy: 0.330000\n",
      "lr 8.977778e-07 reg 2.888889e+04 train accuracy: 0.309551 val accuracy: 0.328000\n",
      "lr 8.977778e-07 reg 3.166667e+04 train accuracy: 0.316633 val accuracy: 0.342000\n",
      "lr 8.977778e-07 reg 3.444444e+04 train accuracy: 0.294653 val accuracy: 0.312000\n",
      "lr 8.977778e-07 reg 3.722222e+04 train accuracy: 0.315735 val accuracy: 0.333000\n",
      "lr 8.977778e-07 reg 4.000000e+04 train accuracy: 0.301898 val accuracy: 0.314000\n",
      "lr 1.000000e-06 reg 1.500000e+04 train accuracy: 0.344265 val accuracy: 0.350000\n",
      "lr 1.000000e-06 reg 1.777778e+04 train accuracy: 0.328327 val accuracy: 0.340000\n",
      "lr 1.000000e-06 reg 2.055556e+04 train accuracy: 0.332694 val accuracy: 0.343000\n",
      "lr 1.000000e-06 reg 2.333333e+04 train accuracy: 0.328449 val accuracy: 0.344000\n",
      "lr 1.000000e-06 reg 2.611111e+04 train accuracy: 0.313857 val accuracy: 0.330000\n",
      "lr 1.000000e-06 reg 2.888889e+04 train accuracy: 0.318388 val accuracy: 0.326000\n",
      "lr 1.000000e-06 reg 3.166667e+04 train accuracy: 0.301327 val accuracy: 0.310000\n",
      "lr 1.000000e-06 reg 3.444444e+04 train accuracy: 0.304224 val accuracy: 0.331000\n",
      "lr 1.000000e-06 reg 3.722222e+04 train accuracy: 0.306571 val accuracy: 0.310000\n",
      "lr 1.000000e-06 reg 4.000000e+04 train accuracy: 0.310327 val accuracy: 0.315000\n",
      "best validation accuracy achieved during cross-validation: 0.359000\n"
     ]
    }
   ],
   "source": [
    "# Use the validation set to tune hyperparameters (regularization strength and\n",
    "# learning rate). You should experiment with different ranges for the learning\n",
    "# rates and regularization strengths; if you are careful you should be able to\n",
    "# get a classification accuracy of over 0.35 on the validation set.\n",
    "from cs6353.classifiers import Softmax\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "learning_rates = np.linspace(8e-8, 1e-6, 10)\n",
    "regularization_strengths = np.linspace(1.5e4, 4e4, 10)\n",
    "# learning_rates = [1e-7, 5e-7]\n",
    "# regularization_strengths = [2.5e4, 5e4]\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Use the validation set to set the learning rate and regularization strength. #\n",
    "# This should be identical to the validation that you did for the SVM; save    #\n",
    "# the best trained softmax classifier in best_softmax.                          #\n",
    "################################################################################\n",
    "\n",
    "for learn in learning_rates:\n",
    "    for regular in regularization_strengths:\n",
    "        new_softmax = Softmax()\n",
    "        new_softmax.train(X_train, y_train, learn, regular, num_iters=1500, verbose=True)\n",
    "        y_train_pred = new_softmax.predict(X_train)\n",
    "        y_val_pred = new_softmax.predict(X_val)\n",
    "        train_acc = np.mean(y_train == y_train_pred)\n",
    "        val_acc = np.mean(y_val == y_val_pred)\n",
    "        if best_val < val_acc:\n",
    "            best_val = val_acc\n",
    "            best_softmax = new_softmax\n",
    "        results[(learn, regular)] = (train_acc, val_acc)\n",
    "\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################\n",
    "    \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy))\n",
    "    \n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T00:33:44.984348800Z",
     "start_time": "2023-09-25T00:33:44.922060400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax on raw pixels final test set accuracy: 0.352000\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "# Evaluate the best softmax on test set\n",
    "y_test_pred = best_softmax.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inline Question** - *True or False*\n",
    "\n",
    "It's possible to add a new data point to a training set that would leave the SVM loss unchanged, but this is not the case with the Softmax classifier loss.\n",
    "\n",
    "*Your answer*: \n",
    "True\n",
    "\n",
    "*Your explanation*:\n",
    "The loss calculated in SVM only considers categories that do not have a score lower than the correct category score Δ, so adding new data points will not change the loss as long as the score of the wrong category is lower than the score Δ of the correct category. But softmax is different. Adding new data points means reducing the score of the correct category, and the loss function of softmax is associated with the relationship between the correct category and the total score, so the loss will change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T00:33:52.046503500Z",
     "start_time": "2023-09-25T00:33:51.525953500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the learned weights for each class\n",
    "w = best_softmax.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    \n",
    "    # Rescale the weights to be between 0 and 255\n",
    "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
